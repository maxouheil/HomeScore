# üìã Plan de Migration : HTML Scraping ‚Üí API

## üéØ Objectif

Migrer progressivement le syst√®me pour utiliser les donn√©es de l'API Jinka au lieu du scraping HTML, am√©liorant ainsi la performance, la stabilit√© et la qualit√© des donn√©es.

---

## üìä √âtat Actuel

### Donn√©es HTML (anciennes)
- **Source** : `scrape_jinka.py` (scraping HTML avec Playwright)
- **Format** : `data/scraped_apartments.json` ou `data/appartements/*.json`
- **Probl√®mes** :
  - Lent (plusieurs minutes pour 42 appartements)
  - Fragile (d√©pendant du HTML/CSS)
  - Consomme beaucoup de ressources (navigateur)
  - Donn√©es parfois incompl√®tes

### Donn√©es API (nouvelles)
- **Source** : `scrape_jinka_api.py` (API Jinka)
- **Format** : `data/scraped_apartments_api_*.json`
- **Avantages** :
  - Rapide (~5 secondes pour 42 appartements)
  - Stable (donn√©es structur√©es)
  - Moins de ressources
  - Donn√©es compl√®tes et fiables

---

## üîÑ Plan de Migration en 4 Phases

### Phase 1 : Compatibilit√© et Unification ‚úÖ (Priorit√© Haute)

**Objectif** : S'assurer que les donn√©es API sont compatibles avec le format existant

**Actions** :
- [x] ‚úÖ Cr√©er `api_data_adapter.py` pour convertir API ‚Üí format scraping
- [x] ‚úÖ Tester la compatibilit√© avec le scoring existant
- [ ] Cr√©er une fonction `load_apartments()` unifi√©e qui charge depuis API ou HTML
- [ ] Cr√©er un script de migration `migrate_to_api_format.py` pour convertir les anciennes donn√©es

**Fichiers √† cr√©er/modifier** :
- `data_loader.py` (nouveau) - Chargeur unifi√© de donn√©es
- `migrate_to_api_format.py` (nouveau) - Migration des anciennes donn√©es

**Dur√©e estim√©e** : 1-2 heures

---

### Phase 2 : Migration du Scraping Principal ‚úÖ (Priorit√© Haute)

**Objectif** : Remplacer le scraping HTML par l'API dans les scripts principaux

**Actions** :
- [x] ‚úÖ Cr√©er `scrape_jinka_api.py` avec interface compatible
- [x] ‚úÖ Tester avec l'alerte RP (42 appartements)
- [ ] Modifier `run_daily_scrape.py` pour utiliser l'API par d√©faut
- [ ] Modifier `scrape.py` pour utiliser l'API
- [ ] Modifier `batch_scraper.py` pour utiliser l'API
- [ ] Conserver le scraping HTML comme fallback optionnel

**Fichiers √† modifier** :
- `run_daily_scrape.py` - Utiliser `JinkaAPIScraper` au lieu de `JinkaScraper`
- `scrape.py` - Ajouter option `--use-api` (d√©faut: True)
- `batch_scraper.py` - Utiliser l'API par d√©faut

**Dur√©e estim√©e** : 2-3 heures

---

### Phase 3 : Migration des Scripts de Traitement (Priorit√© Moyenne)

**Objectif** : Adapter les scripts qui consomment les donn√©es pour utiliser le nouveau format

**Scripts √† migrer** :
- [ ] `scoring.py` - V√©rifier compatibilit√© avec donn√©es API
- [ ] `score_appartement.py` - Adapter si n√©cessaire
- [ ] `generate_html_report.py` - Utiliser nouvelles donn√©es
- [ ] `generate_scorecard_html.py` - Adapter le format
- [ ] `analyze_apartment_style.py` - Utiliser photos API
- [ ] `analyze_photos_unified.py` - Adapter pour photos API
- [ ] `extract_exposition.py` - V√©rifier compatibilit√©

**Fichiers √† modifier** :
- Tous les scripts qui chargent `scraped_apartments.json`
- Utiliser `data_loader.py` unifi√©

**Dur√©e estim√©e** : 3-4 heures

---

### Phase 4 : Optimisation et Nettoyage (Priorit√© Basse)

**Objectif** : Nettoyer le code et optimiser l'utilisation de l'API

**Actions** :
- [ ] Supprimer le code HTML scraping non utilis√© (garder comme fallback)
- [ ] Optimiser le cache API
- [ ] Am√©liorer la gestion d'erreurs API
- [ ] Documenter la nouvelle architecture
- [ ] Cr√©er des tests de r√©gression

**Dur√©e estim√©e** : 2-3 heures

---

## üõ†Ô∏è Impl√©mentation D√©taill√©e

### √âtape 1 : Cr√©er le chargeur de donn√©es unifi√©

**Fichier** : `data_loader.py`

```python
"""
Chargeur unifi√© de donn√©es d'appartements
Supporte √† la fois le format API et HTML
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Optional

def load_apartments(prefer_api: bool = True) -> List[Dict]:
    """
    Charge les appartements depuis API ou HTML
    
    Args:
        prefer_api: Pr√©f√©rer les donn√©es API si disponibles
    
    Returns:
        Liste des appartements
    """
    # Chercher les fichiers API r√©cents
    if prefer_api:
        api_files = sorted(
            Path('data').glob('scraped_apartments_api_*.json'),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        if api_files:
            with open(api_files[0], 'r', encoding='utf-8') as f:
                return json.load(f)
    
    # Fallback sur HTML
    html_file = Path('data/scraped_apartments.json')
    if html_file.exists():
        with open(html_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    return []
```

### √âtape 2 : Modifier `run_daily_scrape.py`

**Changements** :
```python
# Avant
from scrape_jinka import JinkaScraper
scraper = JinkaScraper()

# Apr√®s
from scrape_jinka_api import JinkaAPIScraper
scraper = JinkaAPIScraper()  # Plus rapide et stable
```

### √âtape 3 : Cr√©er un script de migration

**Fichier** : `migrate_to_api_format.py`

```python
"""
Migre les anciennes donn√©es HTML vers le format API
"""

import json
from pathlib import Path
from api_data_adapter import adapt_api_to_scraped_format

def migrate_old_data():
    """Convertit les anciennes donn√©es si n√©cessaire"""
    # Logique de migration si besoin
    pass
```

---

## üìù Checklist de Migration

### Phase 1 : Compatibilit√©
- [ ] Cr√©er `data_loader.py`
- [ ] Tester compatibilit√© scoring avec donn√©es API
- [ ] Cr√©er script de migration

### Phase 2 : Scraping Principal
- [ ] Modifier `run_daily_scrape.py`
- [ ] Modifier `scrape.py`
- [ ] Modifier `batch_scraper.py`
- [ ] Tester le workflow complet

### Phase 3 : Scripts de Traitement
- [ ] Adapter `scoring.py`
- [ ] Adapter `generate_html_report.py`
- [ ] Adapter scripts d'analyse
- [ ] Tester tous les scripts

### Phase 4 : Nettoyage
- [ ] Supprimer code obsol√®te
- [ ] Documenter changements
- [ ] Cr√©er tests

---

## ‚ö†Ô∏è Points d'Attention

### Compatibilit√©
- Les donn√©es API sont d√©j√† compatibles gr√¢ce √† `api_data_adapter.py`
- Tous les champs n√©cessaires sont pr√©sents
- Format identique au scraping HTML

### Fallback
- Garder le scraping HTML comme option de secours
- Ajouter un flag `--use-html` pour forcer HTML si besoin
- G√©rer les erreurs API gracieusement

### Performance
- L'API est 10x plus rapide
- R√©duire les d√©lais dans les scripts batch
- Optimiser le cache API

---

## üéØ R√©sultats Attendus

Apr√®s migration compl√®te :
- ‚úÖ **Performance** : 10x plus rapide (5s vs 50s+)
- ‚úÖ **Stabilit√©** : Moins de breaking changes
- ‚úÖ **Qualit√©** : Donn√©es plus compl√®tes et fiables
- ‚úÖ **Maintenance** : Code plus simple et maintenable

---

## üìÖ Timeline Sugg√©r√©e

- **Semaine 1** : Phase 1 + Phase 2 (compatibilit√© + scraping principal)
- **Semaine 2** : Phase 3 (scripts de traitement)
- **Semaine 3** : Phase 4 (optimisation et nettoyage)

**Total estim√©** : 8-12 heures de travail

---

## üöÄ D√©marrage Rapide

Pour commencer la migration imm√©diatement :

```bash
# 1. Tester le scraper API
python scrape_with_api.py

# 2. V√©rifier les donn√©es
python show_apartment_data.py

# 3. Tester le scoring avec les nouvelles donn√©es
# (√† adapter selon votre workflow)
```

---

**Derni√®re mise √† jour** : 2025-11-14
**Statut** : Phase 1 et 2 compl√©t√©es ‚úÖ

