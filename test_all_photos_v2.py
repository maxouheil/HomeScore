#!/usr/bin/env python3
"""
Script de test pour extraire les photos de TOUS les appartements avec le nouveau syst√®me v2
"""

import asyncio
import json
import os
import requests
from datetime import datetime
from scrape_jinka import JinkaScraper
from dotenv import load_dotenv

load_dotenv()

async def extract_all_photos():
    """Extrait les photos de tous les appartements"""
    scraper = JinkaScraper()
    
    try:
        await scraper.setup()
        
        # Connexion
        print("üîê Connexion √† Jinka...")
        if not await scraper.login():
            print("‚ùå √âchec de la connexion")
            return
        
        print("‚úÖ Connexion r√©ussie\n")
        
        # Charger tous les appartements depuis les scores
        print("üìã Chargement des appartements...")
        try:
            with open('data/scores/all_apartments_scores.json', 'r', encoding='utf-8') as f:
                apartments = json.load(f)
            print(f"‚úÖ {len(apartments)} appartements trouv√©s\n")
        except FileNotFoundError:
            print("‚ùå Fichier all_apartments_scores.json non trouv√©")
            return
        
        results = []
        success_count = 0
        error_count = 0
        
        for i, apartment in enumerate(apartments, 1):
            apartment_id = apartment.get('id', 'unknown')
            apartment_url = apartment.get('url', '')
            
            if not apartment_url:
                print(f"\n{'='*60}")
                print(f"‚ö†Ô∏è  Appartement {i}/{len(apartments)}: {apartment_id} - Pas d'URL")
                print(f"{'='*60}")
                error_count += 1
                continue
            
            print(f"\n{'='*60}")
            print(f"üè† Appartement {i}/{len(apartments)}: {apartment_id}")
            print(f"{'='*60}\n")
            
            try:
                # Naviguer vers l'appartement
                await scraper.page.goto(apartment_url)
                await scraper.page.wait_for_load_state('networkidle')
                await asyncio.sleep(2)  # Attendre le chargement
                
                # Extraire les photos
                photos = await scraper.extract_photos()
                
                print(f"üìä Photos trouv√©es: {len(photos)}")
                
                # Cr√©er le dossier v2
                photos_dir = f"data/photos_v2/{apartment_id}"
                os.makedirs(photos_dir, exist_ok=True)
                
                downloaded_photos = []
                
                for j, photo in enumerate(photos[:5], 1):  # Limiter √† 5 photos max
                    try:
                        # T√©l√©charger la photo
                        response = requests.get(photo['url'], timeout=30)
                        if response.status_code == 200:
                            # Sauvegarder
                            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                            filename = f"{photos_dir}/photo_{j}_{timestamp}.jpg"
                            
                            with open(filename, 'wb') as f:
                                f.write(response.content)
                            
                            file_size = len(response.content)
                            print(f"   ‚úÖ Photo {j}/{len(photos[:5])} t√©l√©charg√©e ({photo.get('width', '?')}x{photo.get('height', '?')}, {file_size:,} bytes)")
                            
                            downloaded_photos.append({
                                'index': j,
                                'url': photo['url'],
                                'selector': photo.get('selector', 'N/A'),
                                'width': photo.get('width', 0),
                                'height': photo.get('height', 0),
                                'filename': filename,
                                'size': file_size
                            })
                        else:
                            print(f"   ‚ùå Erreur t√©l√©chargement photo {j}: {response.status_code}")
                            
                    except Exception as e:
                        print(f"   ‚ùå Erreur photo {j}: {e}")
                        continue
                
                # Sauvegarder les m√©tadonn√©es
                metadata = {
                    'apartment_id': apartment_id,
                    'apartment_url': apartment_url,
                    'total_photos_found': len(photos),
                    'photos_downloaded': len(downloaded_photos),
                    'photos': downloaded_photos,
                    'timestamp': datetime.now().isoformat()
                }
                
                metadata_file = f"{photos_dir}/metadata.json"
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                print(f"üìÑ M√©tadonn√©es sauvegard√©es\n")
                
                results.append(metadata)
                success_count += 1
                
                # Pause entre les appartements pour √©viter de surcharger
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"‚ùå Erreur traitement appartement {apartment_id}: {e}")
                error_count += 1
                continue
        
        # Rapport final
        print(f"\n\n{'='*60}")
        print("üìä RAPPORT FINAL")
        print(f"{'='*60}\n")
        
        print(f"Total appartements: {len(apartments)}")
        print(f"‚úÖ Succ√®s: {success_count}")
        print(f"‚ùå Erreurs: {error_count}\n")
        
        if results:
            total_photos_found = sum(r['total_photos_found'] for r in results)
            total_photos_downloaded = sum(r['photos_downloaded'] for r in results)
            
            print(f"üì∏ Photos trouv√©es au total: {total_photos_found}")
            print(f"üì• Photos t√©l√©charg√©es au total: {total_photos_downloaded}\n")
            
            # Statistiques par appartement
            print("üìä D√©tails par appartement:")
            for result in results:
                print(f"   {result['apartment_id']}: {result['photos_downloaded']}/{result['total_photos_found']} photos")
                
                if result['photos_downloaded'] > 0:
                    widths = [p['width'] for p in result['photos'] if p['width'] > 0]
                    if widths:
                        avg_width = sum(widths) / len(widths)
                        print(f"      Dimensions moyennes: {avg_width:.0f}px de large")
        
        print(f"\n‚úÖ Extraction termin√©e ! V√©rifiez les photos dans data/photos_v2/")
        
        # Sauvegarder le rapport global
        global_report = {
            'total_apartments': len(apartments),
            'success_count': success_count,
            'error_count': error_count,
            'total_photos_found': sum(r['total_photos_found'] for r in results),
            'total_photos_downloaded': sum(r['photos_downloaded'] for r in results),
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
        
        report_file = 'data/photos_v2/global_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(global_report, f, ensure_ascii=False, indent=2)
        
        print(f"üìÑ Rapport global sauvegard√©: {report_file}")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if scraper.browser:
            await scraper.browser.close()

if __name__ == "__main__":
    asyncio.run(extract_all_photos())

