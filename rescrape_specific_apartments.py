#!/usr/bin/env python3
"""
Script pour rescraper des appartements spÃ©cifiques
"""

import asyncio
import json
from scrape_jinka import JinkaScraper

async def rescrape_specific_apartments(apartment_ids):
    """Rescrape des appartements spÃ©cifiques par leur ID"""
    print(f"ğŸ”„ RESCRAPING D'APPARTEMENTS SPÃ‰CIFIQUES")
    print("=" * 60)
    print(f"ğŸ“‹ {len(apartment_ids)} appartements Ã  rescraper: {', '.join(apartment_ids)}\n")
    
    scraper = JinkaScraper()
    
    try:
        await scraper.setup()
        print("âœ… Scraper initialisÃ©")
        
        # Login
        if not await scraper.login():
            print("âŒ Ã‰chec de la connexion")
            return
        
        print("âœ… Connexion rÃ©ussie\n")
        
        # Charger les donnÃ©es existantes pour obtenir les URLs
        apartments_data = []
        for apt_id in apartment_ids:
            try:
                with open(f"data/appartements/{apt_id}.json", 'r', encoding='utf-8') as f:
                    apt_data = json.load(f)
                    apartments_data.append(apt_data)
            except FileNotFoundError:
                print(f"âš ï¸ Fichier non trouvÃ© pour {apt_id}")
                continue
        
        if not apartments_data:
            print("âŒ Aucune donnÃ©e trouvÃ©e")
            return
        
        success_count = 0
        
        for i, apt_data in enumerate(apartments_data, 1):
            apt_id = apt_data.get('id')
            url = apt_data.get('url')
            
            print(f"\nğŸ  [{i}/{len(apartments_data)}] Appartement {apt_id}")
            print(f"   ğŸ“ {apt_data.get('titre', 'N/A')}")
            print(f"   ğŸ’° {apt_data.get('prix', 'N/A')}")
            print(f"   URL: {url}")
            
            if not url:
                print(f"   âš ï¸ Pas d'URL, skip")
                continue
            
            try:
                # Scraper l'appartement
                new_apt_data = await scraper.scrape_apartment(url)
                
                if new_apt_data:
                    photos_count = len(new_apt_data.get('photos', []))
                    
                    if photos_count > 0:
                        print(f"   âœ… {photos_count} photos trouvÃ©es !")
                        success_count += 1
                    else:
                        print(f"   âš ï¸ Toujours aucune photo (peut-Ãªtre vraiment pas de photos disponibles)")
                    
                    # Sauvegarder
                    await scraper.save_apartment(new_apt_data, skip_if_exists=False)
                else:
                    print(f"   âŒ Ã‰chec du scraping")
                
                # Pause entre les requÃªtes
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"   âŒ Erreur: {e}")
        
        print(f"\nğŸ“Š RÃ‰SULTATS:")
        print(f"   âœ… Appartements avec photos rÃ©cupÃ©rÃ©es: {success_count}/{len(apartments_data)}")
        
    except Exception as e:
        print(f"âŒ Erreur globale: {e}")
    finally:
        await scraper.cleanup()

async def main():
    """Fonction principale"""
    # IDs des appartements Ã  rescraper
    apartment_ids = [
        "92913102",  # 707k PyrÃ©nÃ©es
        "92732956",  # 710k Rue des Boulets
    ]
    
    await rescrape_specific_apartments(apartment_ids)

if __name__ == "__main__":
    asyncio.run(main())





