#!/usr/bin/env python3
"""
Scraper en batch pour traiter toutes les annonces d'une alerte Jinka
Optimis√© pour traiter 40+ appartements efficacement
"""

import asyncio
import json
import os
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from scrape_jinka import JinkaScraper
from analyze_apartment_style import ApartmentStyleAnalyzer

class BatchScraper:
    """Scraper en batch optimis√©"""
    
    def __init__(self, max_concurrent=3, max_apartments=50):
        self.max_concurrent = max_concurrent  # Nombre d'appartements en parall√®le
        self.max_apartments = max_apartments  # Limite d'appartements
        self.scraper = JinkaScraper()
        self.style_analyzer = ApartmentStyleAnalyzer()
        self.results = []
        self.errors = []
        
    async def scrape_alert_batch(self, alert_url, pages_to_scrape=5):
        """Scrape toutes les annonces d'une alerte Jinka"""
        print(f"üöÄ D√âMARRAGE DU SCRAPING EN BATCH")
        print(f"=" * 60)
        print(f"URL d'alerte: {alert_url}")
        print(f"Pages √† scraper: {pages_to_scrape}")
        print(f"Appartements max: {self.max_apartments}")
        print(f"Concurrent: {self.max_concurrent}")
        print()
        
        start_time = time.time()
        
        try:
            # 1. Initialiser le scraper
            await self.scraper.setup()
            
            # 2. Connexion √† Jinka
            if not await self.scraper.login():
                print("‚ùå √âchec de la connexion")
                return
            
            # 2. Scraper toutes les pages de l'alerte
            all_apartments = await self.scrape_all_pages(alert_url, pages_to_scrape)
            
            if not all_apartments:
                print("‚ùå Aucun appartement trouv√©")
                return
            
            print(f"‚úÖ {len(all_apartments)} appartements trouv√©s au total")
            
            # 3. Traiter les appartements en batch
            processed_apartments = await self.process_apartments_batch(all_apartments)
            
            # 4. Sauvegarder les r√©sultats
            await self.save_batch_results(processed_apartments)
            
            # 5. Statistiques finales
            elapsed_time = time.time() - start_time
            self.print_final_stats(processed_apartments, elapsed_time)
            
        except Exception as e:
            print(f"‚ùå Erreur globale: {e}")
        finally:
            if hasattr(self.scraper, 'browser') and self.scraper.browser:
                await self.scraper.browser.close()
    
    async def scrape_all_pages(self, alert_url, pages_to_scrape):
        """Scrape toutes les pages de l'alerte"""
        print(f"üìÑ SCRAPING DE TOUTES LES PAGES")
        print("-" * 40)
        
        all_apartments = []
        
        for page in range(1, pages_to_scrape + 1):
            print(f"üìÑ Page {page}/{pages_to_scrape}")
            
            # Construire l'URL de la page
            if '?' in alert_url:
                page_url = f"{alert_url}&page={page}"
            else:
                page_url = f"{alert_url}?page={page}"
            
            try:
                # Scraper la page
                apartments = await self.scraper.scrape_alert_page(page_url)
                
                if apartments:
                    all_apartments.extend(apartments)
                    print(f"   ‚úÖ {len(apartments)} appartements trouv√©s sur la page {page}")
                else:
                    print(f"   ‚ö†Ô∏è Aucun appartement sur la page {page}")
                    # Si pas d'appartements, on peut arr√™ter
                    if page > 1:
                        break
                
                # Limiter le nombre total
                if len(all_apartments) >= self.max_apartments:
                    all_apartments = all_apartments[:self.max_apartments]
                    print(f"   üõë Limite de {self.max_apartments} appartements atteinte")
                    break
                
                # Pause entre les pages pour √©viter la surcharge
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"   ‚ùå Erreur page {page}: {e}")
                continue
        
        return all_apartments
    
    async def process_apartments_batch(self, apartments):
        """Traite les appartements en batch avec concurrence limit√©e"""
        print(f"\nüè† TRAITEMENT EN BATCH DE {len(apartments)} APPARTEMENTS")
        print("-" * 50)
        
        processed = []
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_single_apartment(apartment_url, index):
            async with semaphore:
                try:
                    print(f"üè† Appartement {index+1}/{len(apartments)}")
                    
                    # Scraper les d√©tails de l'appartement
                    apartment_data = await self.scraper.scrape_apartment_details(apartment_url)
                    
                    if apartment_data:
                        # Analyser le style avec les photos
                        style_analysis = await self.analyze_apartment_style_async(apartment_data)
                        if style_analysis:
                            apartment_data['style_analysis'] = style_analysis
                        
                        # Sauvegarder l'appartement individuellement
                        await self.scraper.save_apartment(apartment_data)
                        
                        processed.append(apartment_data)
                        print(f"   ‚úÖ Appartement {index+1} trait√©")
                    else:
                        print(f"   ‚ùå √âchec appartement {index+1}")
                        self.errors.append(f"Appartement {index+1}: {apartment_url}")
                    
                except Exception as e:
                    print(f"   ‚ùå Erreur appartement {index+1}: {e}")
                    self.errors.append(f"Appartement {index+1}: {e}")
        
        # Traiter tous les appartements avec concurrence limit√©e
        tasks = [process_single_apartment(url, i) for i, url in enumerate(apartments)]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        return processed
    
    async def analyze_apartment_style_async(self, apartment_data):
        """Analyse le style de l'appartement de mani√®re asynchrone"""
        try:
            photos = apartment_data.get('photos', [])
            if not photos:
                return None
            
            # Prendre seulement les 3 premi√®res photos pour √©conomiser
            photos_to_analyze = photos[:3]
            
            # Analyser les photos
            analysis = self.style_analyzer.analyze_apartment_photos_from_data(apartment_data)
            return analysis
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur analyse style: {e}")
            return None
    
    async def save_batch_results(self, apartments):
        """Sauvegarde les r√©sultats du batch"""
        print(f"\nüíæ SAUVEGARDE DES R√âSULTATS")
        print("-" * 40)
        
        # Cr√©er le dossier de r√©sultats
        os.makedirs("data/batch_results", exist_ok=True)
        
        # Sauvegarder le r√©sum√©
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary = {
            'timestamp': timestamp,
            'total_apartments': len(apartments),
            'successful': len(apartments),
            'errors': len(self.errors),
            'apartments': []
        }
        
        for apt in apartments:
            summary['apartments'].append({
                'id': apt.get('id'),
                'titre': apt.get('titre'),
                'prix': apt.get('prix'),
                'surface': apt.get('surface'),
                'localisation': apt.get('localisation'),
                'style_analysis': apt.get('style_analysis', {}),
                'exposition': apt.get('exposition', {})
            })
        
        # Sauvegarder le r√©sum√©
        summary_file = f"data/batch_results/summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ R√©sum√© sauvegard√©: {summary_file}")
        
        # Sauvegarder les erreurs si il y en a
        if self.errors:
            errors_file = f"data/batch_results/errors_{timestamp}.json"
            with open(errors_file, 'w', encoding='utf-8') as f:
                json.dump(self.errors, f, indent=2, ensure_ascii=False)
            print(f"‚ö†Ô∏è Erreurs sauvegard√©es: {errors_file}")
    
    def print_final_stats(self, apartments, elapsed_time):
        """Affiche les statistiques finales"""
        print(f"\nüìä STATISTIQUES FINALES")
        print("=" * 60)
        print(f"‚è±Ô∏è Temps total: {elapsed_time:.1f} secondes")
        print(f"üè† Appartements trait√©s: {len(apartments)}")
        print(f"‚ùå Erreurs: {len(self.errors)}")
        print(f"‚ö° Vitesse: {len(apartments)/elapsed_time:.1f} appartements/seconde")
        
        if apartments:
            # Statistiques des styles
            styles = {}
            cuisines_ouvertes = 0
            luminosites = {}
            
            for apt in apartments:
                style_analysis = apt.get('style_analysis', {})
                if style_analysis:
                    style = style_analysis.get('style', {}).get('type', 'inconnu')
                    styles[style] = styles.get(style, 0) + 1
                    
                    if style_analysis.get('cuisine', {}).get('ouverte', False):
                        cuisines_ouvertes += 1
                    
                    luminosite = style_analysis.get('luminosite', {}).get('type', 'inconnue')
                    luminosites[luminosite] = luminosites.get(luminosite, 0) + 1
            
            print(f"\nüìà ANALYSE DES R√âSULTATS:")
            print(f"   Styles d√©tect√©s: {styles}")
            print(f"   Cuisines ouvertes: {cuisines_ouvertes}/{len(apartments)} ({cuisines_ouvertes/len(apartments)*100:.1f}%)")
            print(f"   Luminosit√©s: {luminosites}")

async def main():
    """Fonction principale"""
    # URL de ton alerte Jinka (remplace par la tienne)
    alert_url = "https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad=90931157&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
    
    # Configuration du batch scraper
    batch_scraper = BatchScraper(
        max_concurrent=3,  # 3 appartements en parall√®le
        max_apartments=50  # Maximum 50 appartements
    )
    
    # Lancer le scraping en batch
    await batch_scraper.scrape_alert_batch(alert_url, pages_to_scrape=10)

if __name__ == "__main__":
    asyncio.run(main())
