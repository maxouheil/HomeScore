#!/usr/bin/env python3
"""
Scraper Jinka pour extraire les donn√©es des appartements
"""

import asyncio
import json
import math
import os
import re
import aiohttp
import sys
from datetime import datetime
from playwright.async_api import async_playwright
from dotenv import load_dotenv
from extract_exposition import ExpositionExtractor

load_dotenv()

class JinkaScraper:
    def __init__(self):
        self.browser = None
        self.context = None
        self.page = None
        self.apartments = []
        self.exposition_extractor = ExpositionExtractor()
        
    async def setup(self):
        """Initialise le navigateur et la page"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=False)  # Mode visible
        self.context = await self.browser.new_context()
        self.page = await self.context.new_page()
        
    async def login(self):
        """Se connecte √† Jinka via Google"""
        print("üîê Connexion √† Jinka...")
        
        try:
            await self.page.goto('https://www.jinka.fr/sign/in')
            await self.page.wait_for_load_state('networkidle')
            
            # Cliquer sur "Continuer avec Google"
            google_button = self.page.locator('button:has-text("Continuer avec Google")')
            if await google_button.count() > 0:
                await google_button.click()
                await self.page.wait_for_load_state('networkidle')
                await self.page.wait_for_timeout(2000)
                
                # Saisir l'email
                email_input = self.page.locator('input[type="email"]')
                if await email_input.count() > 0:
                    await email_input.fill(os.getenv('JINKA_EMAIL'))
                    await self.page.keyboard.press('Enter')
                    await self.page.wait_for_timeout(2000)
                
                # Saisir le mot de passe
                password_input = self.page.locator('input[type="password"]')
                if await password_input.count() > 0:
                    await password_input.fill(os.getenv('JINKA_PASSWORD'))
                    await self.page.keyboard.press('Enter')
                    await self.page.wait_for_load_state('networkidle')
                    
                print("‚úÖ Connexion r√©ussie")
                return True
            else:
                print("‚ùå Bouton Google non trouv√©")
                return False
                
        except Exception as e:
            print(f"‚ùå Erreur de connexion: {e}")
            return False
    
    async def scrape_alert_page(self, alert_url):
        """Scrape une page d'alerte Jinka"""
        print(f"üè† Scraping de l'alerte: {alert_url}")
        
        try:
            await self.page.goto(alert_url)
            await self.page.wait_for_load_state('networkidle')
            await self.page.wait_for_timeout(2000)
            
            # Attendre que la page se charge compl√®tement
            await self.page.wait_for_timeout(3000)
            
            # Essayer diff√©rents s√©lecteurs pour les cartes d'appartements
            selectors = [
                'a[href*="alert_result"][href*="ad="]',  # Liens avec alert_result ET ad=
                'a[href*="alert_result"]',
                'a[href*="ad="]',
                'a.sc-bdVaJa.csp.sc-cJSrbW.doPXAe',  # S√©lecteur exact d'apr√®s l'image
                'a.sc-bdVaJa',  # S√©lecteur plus large
                '.apartment-card',
                '[data-testid="apartment-card"]',
                'a[href*="/alert_result"]'
            ]
            
            apartment_links = None
            count = 0
            
            for selector in selectors:
                try:
                    apartment_links = self.page.locator(selector)
                    count = await apartment_links.count()
                    if count > 0:
                        print(f"üìã {count} appartements trouv√©s avec s√©lecteur: {selector}")
                        break
                except:
                    continue
            
            if count == 0:
                print("üîç Aucun appartement trouv√©, debug de la page...")
                # Debug: afficher le contenu de la page
                page_content = await self.page.content()
                print(f"üìÑ Taille de la page: {len(page_content)} caract√®res")
                
                # Chercher tous les liens
                all_links = self.page.locator('a')
                all_links_count = await all_links.count()
                print(f"üîó Total de liens sur la page: {all_links_count}")
                
                # Afficher les premiers liens trouv√©s
                for i in range(min(5, all_links_count)):
                    href = await all_links.nth(i).get_attribute('href')
                    print(f"   Lien {i+1}: {href}")
                
                return False
            
            # Extraire les URLs des appartements
            apartment_urls = []
            for i in range(count):
                href = await apartment_links.nth(i).get_attribute('href')
                print(f"   Lien {i+1}: href='{href}'")
                
                # Chercher les liens avec id= (format loueragile://) ou ad=
                if href and ('id=' in href or 'ad=' in href):
                    # Extraire l'ID de l'appartement
                    apartment_id = None
                    if 'id=' in href:
                        import re
                        match = re.search(r'id=(\d+)', href)
                        if match:
                            apartment_id = match.group(1)
                    elif 'ad=' in href:
                        import re
                        match = re.search(r'ad=(\d+)', href)
                        if match:
                            apartment_id = match.group(1)
                    
                    if apartment_id:
                        # Construire l'URL standard Jinka
                        full_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apartment_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                        apartment_urls.append(full_url)
                        print(f"   ‚úÖ Appartement {i+1} (ID: {apartment_id}): {full_url}")
                    else:
                        print(f"   ‚ùå Lien {i+1} ignor√©: impossible d'extraire l'ID")
                else:
                    print(f"   ‚ùå Lien {i+1} ignor√©: pas de param√®tre 'id=' ou 'ad='")
            
            print(f"üîó {len(apartment_urls)} URLs d'appartements extraites")
            
            # Scraper chaque appartement
            for i, url in enumerate(apartment_urls):
                print(f"üè† Scraping appartement {i+1}/{len(apartment_urls)}")
                apartment_data = await self.scrape_apartment(url)
                if apartment_data:
                    self.apartments.append(apartment_data)
                    await self.save_apartment(apartment_data)
                
                # Pause entre les requ√™tes
                await self.page.wait_for_timeout(1000)
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur scraping alerte: {e}")
            return False
    
    async def scrape_apartment(self, url):
        """Scrape les d√©tails d'un appartement"""
        try:
            await self.page.goto(url)
            await self.page.wait_for_load_state('networkidle')
            await self.page.wait_for_timeout(2000)
            
            # Extraire l'ID de l'appartement
            apartment_id = self.extract_apartment_id(url)
            
            # Extraire les donn√©es
            description = await self.extract_description()
            caracteristiques = await self.extract_caracteristiques()
            photos = await self.extract_photos()
            
            # T√©l√©charger les photos localement
            await self.download_apartment_photos(apartment_id, photos)
            
            data = {
                'id': apartment_id,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'titre': await self.extract_titre(),
                'prix': await self.extract_prix(),
                'prix_m2': await self.extract_prix_m2(),
                'localisation': await self.extract_localisation(),
                'coordinates': await self.extract_coordinates(),
                'map_info': await self.extract_map_info(apartment_id),
                'surface': await self.extract_surface(),
                'pieces': await self.extract_pieces(),
                'date': await self.extract_date(),
                'transports': await self.extract_transports(),
                'description': description,
                'photos': photos,
                'caracteristiques': caracteristiques,
                'agence': await self.extract_agence(),
                'style_haussmannien': await self.extract_style_haussmannien()
            }
            
            # Ajouter l'analyse d'exposition contextuelle
            data['exposition'] = self.exposition_extractor.extract_exposition_ultimate(data)
            
            print(f"‚úÖ Appartement {apartment_id} scrap√©")
            return data
            
        except Exception as e:
            print(f"‚ùå Erreur scraping appartement {url}: {e}")
            return None
    
    def extract_apartment_id(self, url):
        """Extrait l'ID de l'appartement depuis l'URL"""
        match = re.search(r'ad=(\d+)', url)
        return match.group(1) if match else "unknown"
    
    async def extract_titre(self):
        """Extrait le titre de l'appartement"""
        try:
            # Chercher le titre dans diff√©rents s√©lecteurs possibles
            selectors = ['h1', '.title', '[data-testid="title"]', 'h2']
            for selector in selectors:
                element = self.page.locator(selector).first
                if await element.count() > 0:
                    text = await element.text_content()
                    if text and len(text.strip()) > 5:
                        return text.strip()
            return "Titre non trouv√©"
        except:
            return "Titre non trouv√©"
    
    async def extract_prix(self):
        """Extrait le prix principal"""
        try:
            # S√©lecteur pour le prix principal
            price_element = self.page.locator('.hmmXKG, [class*="price"], .price').first
            if await price_element.count() > 0:
                text = await price_element.text_content()
                # Nettoyer le prix
                price = re.search(r'[\d\s]+‚Ç¨', text or '')
                return price.group(0).strip() if price else "Prix non trouv√©"
            return "Prix non trouv√©"
        except:
            return "Prix non trouv√©"
    
    async def extract_prix_m2(self):
        """Extrait le prix au m¬≤"""
        try:
            # Chercher le prix au m¬≤ pr√®s du prix principal
            price_elements = self.page.locator('text=/‚Ç¨\/m¬≤/')
            if await price_elements.count() > 0:
                text = await price_elements.first.text_content()
                if text:
                    # Extraire et formater le prix au m¬≤
                    match = re.search(r'([\d\s]+)\s*‚Ç¨\s*/?\s*m¬≤', text, re.IGNORECASE)
                    if match:
                        prix_clean = match.group(1).strip().replace(' ', ' ')
                        return f"{prix_clean} ‚Ç¨ / m¬≤"
            return None
        except:
            return None
    
    async def extract_etage(self):
        """Extrait l'√©tage de la page d'appartement"""
        try:
            page_content = await self.page.content()
            
            # Patterns pour trouver l'√©tage
            etage_patterns = [
                r'(\d+)(?:er?|e|√®me?)\s*√©tage',
                r'√©tage\s*(\d+)',
                r'(\d+)(?:er?|e|√®me?)\s*√©t\.',
            ]
            
            for pattern in etage_patterns:
                matches = re.findall(pattern, page_content, re.IGNORECASE)
                if matches:
                    etage_num = matches[0]
                    # Formater comme "4e √©tage" ou "1er √©tage"
                    if etage_num == '1':
                        return "1er √©tage"
                    else:
                        return f"{etage_num}e √©tage"
            
            # Chercher RDC
            if re.search(r'RDC|rez-de-chauss√©e|rez de chauss√©e', page_content, re.IGNORECASE):
                return "RDC"
            
            return None
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erreur extraction √©tage: {e}")
            return None
    
    async def extract_style_for_photo(self):
        """Extrait le style de l'appartement pour la description de photo"""
        try:
            page_text = await self.page.text_content('body') or ""
            
            # Chercher des indices de style haussmannien
            style_keywords = {
                'haussmannien': 'Haussmannien',
                'haussmann': 'Haussmannien',
                'moulures': 'Haussmannien',
                'parquet': 'Haussmannien',
                'chemin√©e': 'Haussmannien',
                'restaur√©': 'Haussmannien',
                'contemporain': 'Contemporain',
                'moderne': 'Moderne',
                'ancien': 'Ancien',
                'neuf': 'Neuf'
            }
            
            for keyword, style in style_keywords.items():
                if re.search(keyword, page_text, re.IGNORECASE):
                    return style
            
            return "Style Inconnu"
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erreur extraction style: {e}")
            return "Style Inconnu"
    
    def format_photo_description(self, surface=None, prix_m2=None, etage=None, style=None):
        """Formate la description de photo au format: 70 m¬≤ ¬∑ 3e √©tage ¬∑ Style Inconnu"""
        parts = []
        
        if surface:
            parts.append(surface)
        # Prix au m¬≤ masqu√© pour simplifier
        # if prix_m2:
        #     parts.append(prix_m2)
        if etage:
            parts.append(etage)
        if style:
            parts.append(style)
        
        return " ¬∑ ".join(parts) if parts else "Appartement"
    
    async def extract_localisation(self):
        """Extrait la localisation (adresse exacte si possible)"""
        try:
            # R√©cup√©rer tout le contenu de la page
            page_text = await self.page.text_content('body')
            
            # Chercher l'adresse exacte avec diff√©rents patterns
            address_patterns = [
                r'(\d+[,\s]*[a-zA-Z\s]*[Rr]ue[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Aa]venue[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Bb]oulevard[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Pp]lace[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Cc]ours[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Vv]illa[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Ii]mpasse[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Aa]ll√©e[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Pp]assage[^,\n]*)',
                r'(\d+[,\s]*[a-zA-Z\s]*[Cc]hemin[^,\n]*)'
            ]
            
            adresses_trouvees = []
            for pattern in address_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                for match in matches:
                    # Nettoyer l'adresse
                    clean_addr = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_addr) > 5 and clean_addr not in adresses_trouvees:
                        adresses_trouvees.append(clean_addr)
            
            if adresses_trouvees:
                return adresses_trouvees[0]  # Retourner la premi√®re adresse trouv√©e
            
            # Fallback 1: chercher juste l'arrondissement
            selectors = ['text=/Paris \d+e/', 'text=/750\d+/', '[class*="location"]']
            for selector in selectors:
                element = self.page.locator(selector).first
                if await element.count() > 0:
                    text = await element.text_content()
                    if text and 'Paris' in text:
                        return text.strip()
            
            # Fallback 2: utiliser les stations de m√©tro comme localisation
            try:
                transports = await self.extract_transports()
                if transports:
                    # Prendre les 2 premi√®res stations comme localisation
                    stations_str = ", ".join(transports[:2])
                    return f"Proche de {stations_str}"
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur fallback stations: {e}")
            
            return "Localisation non trouv√©e"
        except:
            return "Localisation non trouv√©e"
    
    async def extract_surface(self):
        """Extrait la surface"""
        try:
            # Chercher la surface dans diff√©rents formats
            surface_elements = self.page.locator('text=/\\d+\\s*m¬≤/')
            if await surface_elements.count() > 0:
                text = await surface_elements.first.text_content()
                if text:
                    # Extraire juste la partie "XX m¬≤"
                    match = re.search(r'(\d+(?:[.,]\d+)?)\s*m¬≤', text, re.IGNORECASE)
                    if match:
                        # Arrondir si d√©cimal et formater
                        surface_val = match.group(1).replace(',', '.')
                        try:
                            surface_num = float(surface_val)
                            return f"{int(surface_num)} m¬≤" if surface_num == int(surface_num) else f"{surface_num:.1f} m¬≤"
                        except:
                            return f"{match.group(1)} m¬≤"
            return None
        except:
            return None
    
    async def extract_pieces(self):
        """Extrait le nombre de pi√®ces"""
        try:
            # Chercher les pi√®ces dans diff√©rents formats
            pieces_elements = self.page.locator('text=/\d+\s*pi√®ces?/')
            if await pieces_elements.count() > 0:
                text = await pieces_elements.first.text_content()
                return text.strip() if text else "Pi√®ces non trouv√©es"
            return "Pi√®ces non trouv√©es"
        except:
            return "Pi√®ces non trouv√©es"
    
    async def extract_date(self):
        """Extrait la date de publication"""
        try:
            # Chercher la date
            date_elements = self.page.locator('text=/le \d+ \w+ √†/')
            if await date_elements.count() > 0:
                text = await date_elements.first.text_content()
                return text.strip() if text else "Date non trouv√©e"
            return "Date non trouv√©e"
        except:
            return "Date non trouv√©e"
    
    async def extract_transports(self):
        """Extrait les transports proches (stations de m√©tro)"""
        try:
            transports = []
            
            # M√©thode 1: Chercher la section "Proche des stations"
            try:
                # Chercher la div qui contient "Proche des stations"
                stations_div = self.page.locator('div.fz-16.sc-bdVaJa.bDXQKW:has(h3:has-text("Proche des stations"))')
                if await stations_div.count() > 0:
                    # Chercher tous les spans dans les li de la liste ul
                    station_spans = stations_div.locator('ul li span')
                    station_count = await station_spans.count()
                    
                    if station_count > 0:
                        for i in range(station_count):
                            station_text = await station_spans.nth(i).text_content()
                            if station_text and len(station_text.strip()) > 2:
                                transports.append(station_text.strip())
                    else:
                        # Fallback: utiliser les li directement
                        station_lis = stations_div.locator('ul li')
                        li_count = await station_lis.count()
                        for i in range(li_count):
                            station_text = await station_lis.nth(i).text_content()
                            if station_text:
                                # Nettoyer le texte (enlever les num√©ros de ligne)
                                station_name = re.sub(r'\s+\d+\s*$', '', station_text.strip())
                                if station_name and len(station_name) > 2:
                                    transports.append(station_name)
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur extraction section stations: {e}")
            
            # M√©thode 2: Chercher les images de m√©tro et extraire les noms
            try:
                metro_images = await self.page.locator('img[src*="subway"], img[alt*="metro"]').all()
                for img in metro_images:
                    # Chercher le texte de la station dans le m√™me conteneur
                    parent = img.locator('..')
                    station_text = await parent.text_content()
                    if station_text:
                        # Extraire le nom de la station (avant les ic√¥nes)
                        station_name = re.split(r'\s+\d+\s*', station_text)[0].strip()
                        if station_name and len(station_name) > 2 and station_name not in transports:
                            transports.append(station_name)
            except Exception as e:
                print(f"  ‚ö†Ô∏è Erreur extraction images m√©tro: {e}")
            
            # M√©thode 3: Fallback - chercher les patterns de stations
            if not transports:
                try:
                    transport_elements = self.page.locator('text=/[A-Za-z]+\s+\d+/')
                    for i in range(await transport_elements.count()):
                        text = await transport_elements.nth(i).text_content()
                        if text and re.match(r'[A-Za-z]+\s+\d+', text.strip()):
                            transports.append(text.strip())
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Erreur extraction fallback: {e}")
            
            # Nettoyer et d√©dupliquer
            transports = list(dict.fromkeys(transports))  # Supprimer les doublons
            return transports[:10]  # Limiter √† 10 transports
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erreur extraction transports: {e}")
            return []
    
    async def extract_description(self):
        """Extrait la description d√©taill√©e"""
        try:
            # Essayer diff√©rents s√©lecteurs pour la description
            description_selectors = [
                '.fz-16.sc-bxivhb.fcnykg',
                '[class*="description"]',
                'p:has-text("Globalstone")',
                'text=/Globalstone/',
                'div:has-text("Globalstone")',
                'section:has-text("Globalstone")'
            ]
            
            for selector in description_selectors:
                try:
                    element = self.page.locator(selector).first
                    if await element.count() > 0:
                        text = await element.text_content()
                        if text and len(text.strip()) > 50:  # S'assurer qu'on a une vraie description
                            return text.strip()
                except:
                    continue
            
            return "Description non trouv√©e"
        except:
            return "Description non trouv√©e"
    
    async def extract_map_info(self, apartment_id=None):
        """Extrait les informations de la carte (rues, quartier, m√©tros)"""
        try:
            print("   üó∫Ô∏è Analyse de la carte...")
            
            # Initialiser screenshot_path
            screenshot_path = None
            
            # Prendre un screenshot de la carte pour analyse
            map_element = self.page.locator('.leaflet-container, [class*="map"], [class*="carte"]').first
            if await map_element.count() > 0:
                # Attendre que la carte se charge compl√®tement pour cet appartement
                # Attendre que les tuiles de la carte soient charg√©es
                await self.page.wait_for_timeout(1000)
                
                # Attendre que la carte soit visible et charg√©e
                try:
                    await map_element.wait_for(state='visible', timeout=5000)
                except:
                    pass
                
                # Attendre un peu plus pour que la carte se centre sur l'appartement
                await self.page.wait_for_timeout(2000)
                
                # Scroller vers la carte pour s'assurer qu'elle est visible
                try:
                    await map_element.scroll_into_view_if_needed()
                    await self.page.wait_for_timeout(1000)
                except:
                    pass
                
                # Prendre un screenshot de la carte avec l'ID de l'appartement dans le nom
                if apartment_id:
                    screenshot_path = f"data/screenshots/map_{apartment_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                else:
                    screenshot_path = f"data/screenshots/map_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                os.makedirs("data/screenshots", exist_ok=True)
                await map_element.screenshot(path=screenshot_path)
                print(f"   üì∏ Screenshot de la carte sauvegard√©: {screenshot_path}")
            
            # Extraire le texte visible sur la carte
            map_text = ""
            try:
                map_text = await map_element.text_content()
            except:
                pass
            
            # Chercher des noms de rues dans le contenu de la page
            page_content = await self.page.text_content('body')
            
            # Patterns pour identifier les rues et quartiers
            street_patterns = [
                r'Rue\s+[A-Za-z\s\-\']+',
                r'Avenue\s+[A-Za-z\s\-\']+',
                r'Boulevard\s+[A-Za-z\s\-\']+',
                r'Place\s+[A-Za-z\s\-\']+',
                r'Cours\s+[A-Za-z\s\-\']+',
                r'Villa\s+[A-Za-z\s\-\']+',
                r'Impasse\s+[A-Za-z\s\-\']+',
                r'All√©e\s+[A-Za-z\s\-\']+',
                r'Passage\s+[A-Za-z\s\-\']+',
                r'Chemin\s+[A-Za-z\s\-\']+'
            ]
            
            metro_patterns = [
                r'[A-Za-z\s\-\']+\s*\(m√©tro\)',
                r'Station\s+[A-Za-z\s\-\']+',
                r'M√©tro\s+[A-Za-z\s\-\']+'
            ]
            
            # Extraire les rues
            streets_found = []
            for pattern in street_patterns:
                matches = re.findall(pattern, page_content, re.IGNORECASE)
                for match in matches:
                    clean_street = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_street) > 5 and clean_street not in streets_found:
                        streets_found.append(clean_street)
            
            # Extraire les m√©tros
            metros_found = []
            for pattern in metro_patterns:
                matches = re.findall(pattern, page_content, re.IGNORECASE)
                for match in matches:
                    clean_metro = re.sub(r'\s+', ' ', match.strip())
                    if len(clean_metro) > 3 and clean_metro not in metros_found:
                        metros_found.append(clean_metro)
            
            # Identifier le quartier bas√© sur les rues trouv√©es
            quartier = self.identify_quartier(streets_found, metros_found)
            
            map_info = {
                "streets": streets_found[:10],  # Limiter √† 10 rues
                "metros": metros_found[:5],     # Limiter √† 5 m√©tros
                "quartier": quartier,
                "screenshot": screenshot_path if 'screenshot_path' in locals() else None
            }
            
            print(f"   üèòÔ∏è Quartier identifi√©: {quartier}")
            print(f"   üõ£Ô∏è Rues trouv√©es: {len(streets_found)}")
            print(f"   üöá M√©tros trouv√©s: {len(metros_found)}")
            
            return map_info
            
        except Exception as e:
            print(f"   ‚ùå Erreur analyse carte: {e}")
            return {"streets": [], "metros": [], "quartier": "Non identifi√©", "error": str(e)}
    
    def identify_quartier(self, streets, metros):
        """Identifie le quartier bas√© sur les rues et m√©tros trouv√©s"""
        # Quartiers du 19e avec leurs rues caract√©ristiques
        quartiers_19e = {
            "Buttes-Chaumont": ["Rue Botzaris", "Avenue Secr√©tan", "Rue Manin", "Rue de Crim√©e", "Botzaris", "Secr√©tan", "Manin", "Crim√©e"],
            "Place des F√™tes": ["Rue Carducci", "Rue M√©lingue", "Rue des Alouettes", "Rue de la Villette", "Carducci", "M√©lingue", "Alouettes", "Villette"],
            "Jourdain": ["Rue de Belleville", "Rue Pelleport", "Rue de Mouza√Øa", "Rue Compans", "Belleville", "Pelleport", "Mouza√Øa", "Compans"],
            "Pyr√©n√©es": ["Rue Pradier", "Rue Clavel", "Rue R√©beval", "Rue Levert", "Pradier", "Clavel", "R√©beval", "Levert"],
            "Belleville": ["Rue de Belleville", "Rue du Faubourg du Temple", "Boulevard de la Villette", "Belleville", "Faubourg du Temple", "Villette"],
            "Canal de l'Ourcq": ["Quai de la Loire", "Quai de la Seine", "Rue de l'Ourcq", "Loire", "Seine", "Ourcq"]
        }
        
        # M√©tros caract√©ristiques
        metros_quartiers = {
            "Place des F√™tes": ["Place des F√™tes", "Place des Fetes"],
            "Jourdain": ["Jourdain"],
            "Pyr√©n√©es": ["Pyr√©n√©es", "Pyrenees"],
            "Buttes-Chaumont": ["Botzaris", "Crim√©e", "Crim√©e"],
            "Belleville": ["Belleville", "Couronnes"]
        }
        
        # Compter les correspondances
        quartier_scores = {}
        
        for quartier, rues_quartier in quartiers_19e.items():
            score = 0
            for rue in streets:
                for rue_quartier in rues_quartier:
                    if rue_quartier.lower() in rue.lower():
                        score += 1
            quartier_scores[quartier] = score
        
        # Ajouter les scores des m√©tros
        for quartier, metros_quartier in metros_quartiers.items():
            for metro in metros:
                for metro_quartier in metros_quartier:
                    if metro_quartier.lower() in metro.lower():
                        quartier_scores[quartier] = quartier_scores.get(quartier, 0) + 2
        
        # Retourner le quartier avec le plus haut score
        if quartier_scores:
            best_quartier = max(quartier_scores, key=quartier_scores.get)
            if quartier_scores[best_quartier] > 0:
                return f"{best_quartier} (score: {quartier_scores[best_quartier]})"
        
        return "Quartier non identifi√©"
    
    def analyze_screenshot_for_quartier(self, screenshot_path):
        """Analyse le screenshot pour identifier le quartier (m√©thode basique)"""
        try:
            if not os.path.exists(screenshot_path):
                return "Screenshot non trouv√©"
            
            # Pour l'instant, on retourne une analyse bas√©e sur la description
            # Dans une version avanc√©e, on pourrait utiliser OCR ou vision par ordinateur
            return "Analyse manuelle requise - voir screenshot"
            
        except Exception as e:
            return f"Erreur analyse: {e}"
    
    async def extract_coordinates(self):
        """Extrait les coordonn√©es GPS depuis la carte Leaflet"""
        try:
            # Chercher les √©l√©ments de la carte Leaflet avec diff√©rents s√©lecteurs
            leaflet_selectors = [
                '.leaflet-proxy',
                '.leaflet-map-pane', 
                '.leaflet-container',
                '[class*="leaflet"]'
            ]
            
            coordinates = None
            for selector in leaflet_selectors:
                elements = self.page.locator(selector)
                count = await elements.count()
                
                for i in range(count):
                    element = elements.nth(i)
                    style = await element.get_attribute('style')
                    
                    if style and 'translate3d' in style:
                        print(f"   üîç Style trouv√©: {style[:100]}...")
                        
                        # Extraire les coordonn√©es du transform avec regex plus robuste
                        import re
                        patterns = [
                            r'translate3d\(([^,]+),\s*([^,]+),\s*([^)]+)\)',
                            r'translate3d\(([^,]+),\s*([^,]+),\s*([^)]+)\)',
                            r'transform:\s*translate3d\(([^,]+),\s*([^,]+),\s*([^)]+)\)'
                        ]
                        
                        for pattern in patterns:
                            match = re.search(pattern, style)
                            if match:
                                try:
                                    x_str = match.group(1).strip()
                                    y_str = match.group(2).strip()
                                    scale_str = match.group(3).strip()
                                    
                                    print(f"   üìç Coordonn√©es brutes: x={x_str}, y={y_str}, scale={scale_str}")
                                    
                                    # Nettoyer et convertir les valeurs
                                    x = float(x_str.replace('px', '').replace('e+', 'e'))
                                    y = float(y_str.replace('px', '').replace('e+', 'e'))
                                    scale = float(scale_str.replace('px', '')) if scale_str != '0px' else 1.0
                                    
                                    # V√©rifier que les valeurs sont valides (pas 0)
                                    if abs(x) > 1000 and abs(y) > 1000:  # Coordonn√©es Web Mercator valides
                                        # Convertir les coordonn√©es Web Mercator en lat/lng
                                        lon = (x / 20037508.34) * 180
                                        lat = (y / 20037508.34) * 180
                                        lat = 180 / 3.14159265359 * (2 * math.atan(math.exp(lat * 3.14159265359 / 180)) - 3.14159265359 / 2)
                                        
                                        coordinates = {
                                            "latitude": round(lat, 6),
                                            "longitude": round(lon, 6),
                                            "raw_x": x,
                                            "raw_y": y,
                                            "scale": scale
                                        }
                                        print(f"   ‚úÖ Coordonn√©es converties: {lat:.6f}, {lon:.6f}")
                                        break
                                    else:
                                        print(f"   ‚ö†Ô∏è Coordonn√©es invalides (trop petites): x={x}, y={y}")
                                        
                                except ValueError as ve:
                                    print(f"   ‚ùå Erreur de conversion: {ve}")
                                    continue
                        
                        if coordinates:
                            break
                
                if coordinates:
                    break
            
            if not coordinates:
                print("   ‚ùå Aucune coordonn√©e valide trouv√©e")
                return {"latitude": None, "longitude": None, "error": "No valid coordinates found"}
            
            return coordinates
            
        except Exception as e:
            print(f"   ‚ùå Erreur g√©n√©rale: {e}")
            return {"latitude": None, "longitude": None, "error": str(e)}
    
    async def extract_style_haussmannien(self):
        """Extrait les √©l√©ments de style haussmannien"""
        try:
            # R√©cup√©rer la description
            description = await self.extract_description()
            if description == "Description non trouv√©e":
                return {"score": 0, "elements": [], "keywords": []}
            
            # Mots-cl√©s haussmanniens √©tendus
            haussmann_keywords = {
                'architectural': [
                    'haussmannien', 'haussmannienne', 'haussmann',
                    'moulures', 'moulure', 'moulur√©', 'moulur√©e',
                    'chemin√©e', 'chemin√©es', 'chemin√©e de marbre',
                    'parquet', 'parquets', 'parquet d\'origine', 'parquet ancien',
                    'corniches', 'corniche', 'corniche moulur√©e',
                    'rosaces', 'rosace', 'rosace de plafond',
                    'balcon', 'balcons', 'balcon en fer forg√©', 'balcon forg√©',
                    'fer forg√©', 'fer forg√©e', 'grille en fer forg√©',
                    'hauteur sous plafond', 'haut plafond', 'plafond haut',
                    'escalier', 'escaliers', 'escalier d\'honneur'
                ],
                'caract√®re': [
                    'caract√®re', 'caract√®res', 'caract√©ristique',
                    'restaur√©', 'restaur√©e', 'r√©nov√©', 'r√©nov√©e',
                    'authentique', 'authentiques', 'original', 'originale',
                    '√©poque', 'p√©riode', '√©poque haussmannienne',
                    'ancien', 'ancienne', 'ancien immeuble',
                    'vieux', 'vieille', 'vieux immeuble',
                    'charme', 'charmant', 'charmante',
                    'prestige', 'prestigieux', 'prestigieuse',
                    'noble', 'noblesse', 'noblesse des mat√©riaux'
                ],
                'mat√©riaux': [
                    'marbre', 'marbres', 'marbre de carrare',
                    'bois', 'bois noble', 'bois pr√©cieux',
                    'pierre', 'pierres', 'pierre de taille',
                    'stuc', 'stucs', 'stuc d√©coratif',
                    'pl√¢tre', 'pl√¢tres', 'pl√¢tre moul√©',
                    'm√©tal', 'm√©taux', 'm√©tal forg√©'
                ],
                'd√©tails': [
                    'moulure', 'moulures', 'moulure de plafond',
                    'd√©coration', 'd√©coratif', 'd√©corative',
                    'ornement', 'ornements', 'ornemental',
                    'd√©tail', 'd√©tails', 'd√©tail architectural',
                    'finesse', 'finesses', 'finitions',
                    '√©l√©gance', '√©l√©gant', '√©l√©gante'
                ]
            }
            
            # Chercher les mots-cl√©s par cat√©gorie
            found_by_category = {}
            total_found = 0
            all_keywords = []
            
            for category, keywords in haussmann_keywords.items():
                found_in_category = []
                for keyword in keywords:
                    if keyword.lower() in description.lower():
                        found_in_category.append(keyword)
                        all_keywords.append(keyword)
                        total_found += 1
                
                if found_in_category:
                    found_by_category[category] = found_in_category
            
            # Calculer un score de style
            style_score = min(100, (total_found * 10) + 20)  # 10 points par mot-cl√© + 20 de base
            
            return {
                "score": style_score,
                "elements": found_by_category,
                "keywords": all_keywords,
                "total_found": total_found
            }
            
        except Exception as e:
            return {"score": 0, "elements": [], "keywords": [], "error": str(e)}
    
    async def extract_photos(self):
        """Extrait les URLs des photos d'appartement depuis la div sp√©cifique"""
        try:
            print("   üì∏ Extraction des photos d'appartement...")
            
            # Extraire les informations pour la description des photos
            etage = await self.extract_etage()
            surface = await self.extract_surface()
            prix_m2 = await self.extract_prix_m2()
            style = await self.extract_style_for_photo()
            
            if etage:
                print(f"      üè¢ √âtage trouv√©: {etage}")
            if surface:
                print(f"      üìê Surface trouv√©e: {surface}")
            if prix_m2:
                print(f"      üí∞ Prix au m¬≤ trouv√©: {prix_m2}")
            if style:
                print(f"      üé® Style trouv√©: {style}")
            
            photos = []
            
            # Attendre un peu plus longtemps pour le chargement des images lazy
            await asyncio.sleep(1)
            
            # Scroller un peu pour d√©clencher le chargement lazy si n√©cessaire
            await self.page.evaluate('window.scrollTo(0, 200)')
            await asyncio.sleep(0.5)
            await self.page.evaluate('window.scrollTo(0, 0)')
            await asyncio.sleep(0.5)
            
            # M√©thode 1: Cibler la div galerie principale (sc-cJSrbW juBoVb ou sc-gPEVay jnWxBz)
            # Aussi chercher dans les divs cach√©es avec display="none" qui contiennent toutes les photos
            gallery_selectors = [
                'div.sc-cJSrbW.juBoVb',  # Structure actuelle visible dans l'image
                'div.sc-gPEVay.jnWxBz',  # Ancienne structure
                '[class*="sc-cJSrbW"][class*="juBoVb"]',  # S√©lecteurs partiels
                '[class*="sc-gPEVay"][class*="jnWxBz"]',
                'div.sc-bdVaJa.InsofV',  # Div cach√©e avec toutes les photos (display="none")
                '[class*="sc-bdVaJa"][class*="InsofV"]',  # S√©lecteur partiel
                'div[style*="display: none"]',  # Toute div cach√©e
            ]
            
            gallery_found = False
            for selector in gallery_selectors:
                try:
                    gallery_div = self.page.locator(selector)
                    if await gallery_div.count() > 0:
                        print(f"      üéØ Div galerie trouv√©e ({selector}), extraction des images visibles...")
                        gallery_found = True
                        
                        # Extraire toutes les images de la galerie (visibles ET cach√©es avec preloader)
                        gallery_element = await gallery_div.first.element_handle()
                        img_elements = await gallery_element.evaluate('''
                            el => {
                                // Obtenir toutes les images dans l'ordre exact du DOM (m√™me cach√©es)
                                const allImgs = Array.from(el.querySelectorAll('img'));
                                
                                // Extraire les infos avec position visuelle pour tri correct
                                return allImgs.map((img, domIndex) => {
                                    const rect = img.getBoundingClientRect();
                                    const computedStyle = window.getComputedStyle(img);
                                    return {
                                        domIndex: domIndex,  // Index dans le DOM
                                        src: img.src || img.getAttribute('data-src') || img.getAttribute('data-lazy-src') || '',
                                        alt: img.alt || '',
                                        width: img.naturalWidth || img.width || 0,
                                        height: img.naturalHeight || img.height || 0,
                                        display: computedStyle.display,
                                        visibility: computedStyle.visibility,
                                        top: rect.top,  // Position top pour tri visuel
                                        left: rect.left  // Position left pour tri visuel
                                    };
                                }).filter(img => {
                                    // Garder toutes les images avec une URL valide
                                    // Y compris celles avec display="none" et alt="preloader" si elles ont une URL de photo valide
                                    if (!img.src) return false;
                                    if (img.src.toLowerCase().includes('placeholder')) return false;
                                    
                                    // Patterns de vraies photos d'appartements
                                    const photoPatterns = ['loueragile', 'upload_pro_ad', 'media.apimo.pro', 'studio-net.fr', 'images.century21.fr', 'biens', 'apartement', 'transopera', 'staticlbi', 'uploadcaregdc', 'uploadcare', 's3.amazonaws.com', 'googleusercontent.com', 'cdn.safti.fr', 'safti.fr', 'paruvendu.fr', 'immo-facile.com', 'mms.seloger.com', 'seloger.com'];
                                    const hasValidPhotoPattern = photoPatterns.some(pattern => img.src.toLowerCase().includes(pattern));
                                    
                                    // Si c'est une vraie photo, on la garde m√™me si cach√©e ou avec alt="preloader"
                                    return hasValidPhotoPattern;
                                });
                            }
                        ''')
                        
                        # Extraire les photos avec leur index DOM pour pr√©server l'ordre de Jinka
                        photos_with_position = []
                        for img_data in img_elements:
                            try:
                                src_to_use = img_data.get('src', '')
                                if not src_to_use:
                                    continue
                                
                                # V√©rifier que c'est une vraie photo (pas un logo)
                                if 'logo' in src_to_use.lower() or 'source_logos' in src_to_use.lower():
                                    continue
                                
                                # Accepter les URLs de vraies photos d'appartements (patterns √©tendus)
                                photo_patterns = ['loueragile', 'upload_pro_ad', 'media.apimo.pro', 'studio-net.fr', 'images.century21.fr', 'biens', 'apartement', 'transopera', 'staticlbi', 'uploadcaregdc', 'uploadcare', 's3.amazonaws.com', 'googleusercontent.com', 'cdn.safti.fr', 'safti.fr', 'paruvendu.fr', 'immo-facile.com', 'mms.seloger.com', 'seloger.com']
                                if not any(pattern in src_to_use.lower() for pattern in photo_patterns):
                                    continue
                                
                                # V√©rifier les dimensions de l'image (exclure les tr√®s petites comme les logos)
                                width = img_data.get('width', 0)
                                height = img_data.get('height', 0)
                                
                                # Les logos font g√©n√©ralement ~128x128px, les vraies photos sont beaucoup plus grandes
                                # On exclut seulement les images tr√®s petites (< 200px)
                                if width > 0 and height > 0:
                                    if width < 200 or height < 200:
                                        # Probablement un logo ou ic√¥ne (ex: logo immobilier 128x128), on skip
                                        continue
                                
                                # Formater la description compl√®te avec toutes les infos
                                alt = self.format_photo_description(surface, prix_m2, etage, style)
                                
                                position_top = img_data.get('top', 0)
                                position_left = img_data.get('left', 0)
                                
                                # Garder toutes les photos valides, m√™me si cach√©es (display="none")
                                # Les photos avec alt="preloader" dans des divs cach√©es sont souvent les vraies photos
                                
                                photos_with_position.append({
                                    'url': src_to_use,
                                    'alt': alt or 'appartement',
                                    'selector': 'gallery_div_visible',
                                    'width': width,
                                    'height': height,
                                    'dom_index': img_data.get('domIndex', 0),
                                    'position_top': position_top,
                                    'position_left': position_left
                                })
                            except Exception as e:
                                continue
                        
                        # D√©dupliquer par URL, en gardant la photo avec la meilleure position (visible de pr√©f√©rence)
                        url_to_photo = {}
                        for photo in photos_with_position:
                            url = photo['url']
                            top = photo.get('position_top', 0)
                            left = photo.get('position_left', 0)
                            
                            # Si on a d√©j√† cette URL
                            if url in url_to_photo:
                                existing_top = url_to_photo[url].get('position_top', 0)
                                existing_left = url_to_photo[url].get('position_left', 0)
                                
                                # Pr√©f√©rer la photo avec position non-0,0 (visible)
                                if (top != 0 or left != 0) and (existing_top == 0 and existing_left == 0):
                                    # Nouvelle photo est visible, remplacer
                                    url_to_photo[url] = photo
                                elif (top == 0 and left == 0) and (existing_top != 0 or existing_left != 0):
                                    # Photo existante est visible, garder celle-l√†
                                    pass
                                else:
                                    # Les deux ont m√™me type de position, garder la premi√®re
                                    pass
                            else:
                                # Premi√®re occurrence de cette URL
                                url_to_photo[url] = photo
                        
                        photos_with_position = list(url_to_photo.values())
                        
                        # S√©parer les photos visibles (position != 0,0) des photos cach√©es (0,0)
                        visible_photos = [p for p in photos_with_position if p.get('position_top', 0) != 0 or p.get('position_left', 0) != 0]
                        hidden_photos = [p for p in photos_with_position if p.get('position_top', 0) == 0 and p.get('position_left', 0) == 0]
                        
                        # Trier les photos visibles par position (top puis left) pour l'ordre visuel
                        visible_photos.sort(key=lambda x: (x.get('position_top', 0), x.get('position_left', 0)))
                        
                        # Trier les photos cach√©es par index DOM pour pr√©server l'ordre de Jinka
                        hidden_photos.sort(key=lambda x: x.get('dom_index', 0))
                        
                        # Combiner : photos visibles d'abord, puis photos cach√©es dans l'ordre DOM
                        photos_with_position = visible_photos + hidden_photos
                        print(f"      ‚úÖ {len(visible_photos)} photos visibles + {len(hidden_photos)} photos cach√©es = {len(photos_with_position)} photos au total")
                        
                        # Ajouter les photos dans l'ordre correct (ordre visuel de Jinka)
                        for photo_with_pos in photos_with_position:
                            photo = {k: v for k, v in photo_with_pos.items() if k not in ['dom_index', 'position_top', 'position_left']}
                            photos.append(photo)
                            print(f"      üì∏ Photo galerie (top: {photo_with_pos.get('position_top', 0):.0f}, left: {photo_with_pos.get('position_left', 0):.0f}, {photo_with_pos['width']}x{photo_with_pos['height']}): {photo_with_pos['url'][:60]}...")
                        
                        if len(photos) > 0:
                            # Ne pas break, continuer √† chercher dans d'autres s√©lecteurs pour accumuler toutes les photos
                            pass
                except Exception as e:
                    continue
            
            # Apr√®s avoir cherch√© dans toutes les galeries, d√©dupliquer
            if len(photos) > 0:
                unique_photos_temp = []
                seen_urls_temp = set()
                for photo in photos:
                    if photo['url'] not in seen_urls_temp:
                        unique_photos_temp.append(photo)
                        seen_urls_temp.add(photo['url'])
                photos = unique_photos_temp
                print(f"      ‚úÖ {len(photos)} photos uniques trouv√©es apr√®s d√©duplication")
            
            # M√©thode 2: Si pas de photos dans la galerie, chercher les images visibles avec URLs d'appartement
            if len(photos) == 0:
                print("      ‚ö†Ô∏è Aucune photo dans la galerie, recherche d'images visibles...")
                
                # Attendre un peu pour que les images lazy-loaded se chargent
                await asyncio.sleep(2)
                
                # Chercher UNIQUEMENT les images visibles avec URLs d'appartement
                all_visible_images = await self.page.locator('img:visible').all()
                print(f"      üîç {len(all_visible_images)} images visibles totales sur la page")
                
                # M√©thode 2a: Chercher dans les images visibles
                for img in all_visible_images:
                    try:
                        # V√©rifier que l'image est vraiment visible
                        display = await img.evaluate('el => window.getComputedStyle(el).display')
                        if display == 'none':
                            continue
                        
                        # V√©rifier les dimensions de l'image (exclure les tr√®s petites comme les logos)
                        width = await img.evaluate('el => el.naturalWidth || el.width || 0')
                        height = await img.evaluate('el => el.naturalHeight || el.height || 0')
                        
                        # Les logos font g√©n√©ralement ~128x128px, les vraies photos sont beaucoup plus grandes
                        # On exclut seulement les images tr√®s petites (< 200px)
                        if width > 0 and height > 0:
                            if width < 200 or height < 200:
                                # Probablement un logo ou ic√¥ne (ex: logo immobilier 128x128), on skip
                                continue
                        
                        src = await img.get_attribute('src')
                        # Chercher aussi dans data-src (lazy loading)
                        data_src = await img.get_attribute('data-src')
                        src_to_use = src or data_src
                        
                        # Accepter les URLs de vraies photos d'appartements (patterns √©tendus)
                        photo_patterns = ['loueragile', 'upload_pro_ad', 'media.apimo.pro', 'studio-net.fr', 'images.century21.fr', 'biens', 'apartement', 'transopera', 'staticlbi', 'uploadcaregdc', 'uploadcare', 's3.amazonaws.com', 'googleusercontent.com', 'cdn.safti.fr', 'safti.fr', 'paruvendu.fr', 'immo-facile.com', 'mms.seloger.com', 'seloger.com']
                        if src_to_use and any(pattern in src_to_use.lower() for pattern in photo_patterns):
                            # Exclure les logos
                            if 'logo' in src_to_use.lower() or 'source_logos' in src_to_use.lower() or 'preloader' in src_to_use.lower():
                                continue
                            
                            # Formater la description compl√®te avec toutes les infos
                            alt = self.format_photo_description(surface, prix_m2, etage, style)
                            
                            photos.append({
                                'url': src_to_use,
                                'alt': alt or 'appartement',
                                'selector': 'global_search_visible',
                                'width': width,
                                'height': height
                            })
                            print(f"      üì∏ Photo visible ({width}x{height}): {src_to_use[:60]}...")
                    except Exception as e:
                        continue
                
                # M√©thode 2b: Si toujours rien, chercher dans toutes les images (m√™me cach√©es, au cas o√π)
                if len(photos) == 0:
                    print("      üîç Recherche alternative dans toutes les images (y compris lazy-loaded)...")
                    all_images = await self.page.locator('img').all()
                    
                    for img in all_images:
                        try:
                            # R√©cup√©rer src et data-src (pour lazy loading)
                            src = await img.get_attribute('src')
                            data_src = await img.get_attribute('data-src')
                            data_lazy = await img.get_attribute('data-lazy-src')
                            src_to_use = src or data_src or data_lazy
                            
                            if not src_to_use:
                                continue
                            
                            # Filtrer par URL (patterns √©tendus)
                            photo_patterns = ['loueragile', 'upload_pro_ad', 'media.apimo.pro', 'studio-net.fr', 'images.century21.fr', 'biens', 'apartement', 'transopera', 'staticlbi', 'uploadcaregdc', 'uploadcare', 's3.amazonaws.com', 'googleusercontent.com', 'cdn.safti.fr', 'safti.fr', 'paruvendu.fr', 'immo-facile.com', 'mms.seloger.com', 'seloger.com']
                            if not any(pattern in src_to_use.lower() for pattern in photo_patterns):
                                continue
                            
                            # Exclure les logos et preloaders
                            if 'logo' in src_to_use.lower() or 'source_logos' in src_to_use.lower() or 'preloader' in src_to_use.lower():
                                continue
                            
                            # V√©rifier les dimensions si l'image est charg√©e
                            width = 0
                            height = 0
                            try:
                                width = await img.evaluate('el => el.naturalWidth || el.width || 0')
                                height = await img.evaluate('el => el.naturalHeight || el.height || 0')
                                
                                if width > 0 and height > 0 and (width < 200 or height < 200):
                                    continue
                            except:
                                pass  # Si l'image n'est pas encore charg√©e, on garde quand m√™me
                            
                            photos.append({
                                'url': src_to_use,
                                'alt': await img.get_attribute('alt') or 'appartement',
                                'selector': 'global_search_all',
                                'width': width,
                                'height': height
                            })
                            print(f"      üì∏ Photo trouv√©e (lazy-loaded?): {src_to_use[:60]}...")
                        except Exception as e:
                            continue
            
            # D√©dupliquer
            unique_photos = []
            seen_urls = set()
            for photo in photos:
                if photo['url'] not in seen_urls:
                    unique_photos.append(photo)
                    seen_urls.add(photo['url'])
            
            print(f"   ‚úÖ {len(unique_photos)} photos d'appartement trouv√©es")
            return unique_photos  # Retourner toutes les photos disponibles
            
        except Exception as e:
            print(f"   ‚ùå Erreur extraction photos: {e}")
            return []
    
    async def extract_caracteristiques(self):
        """Extrait les caract√©ristiques"""
        try:
            # Chercher la section caract√©ristiques
            char_elements = self.page.locator('h3:has-text("Caract√©ristiques") + *')
            if await char_elements.count() > 0:
                text = await char_elements.first.text_content()
                return text.strip() if text else "Caract√©ristiques non trouv√©es"
            return "Caract√©ristiques non trouv√©es"
        except:
            return "Caract√©ristiques non trouv√©es"
    
    async def extract_agence(self):
        """Extrait les informations de l'agence"""
        try:
            # Chercher le nom de l'agence
            agence_elements = self.page.locator('text=/[A-Z][A-Z\s]+/')
            for i in range(await agence_elements.count()):
                text = await agence_elements.nth(i).text_content()
                if text and len(text.strip()) > 3 and text.isupper():
                    return text.strip()
            return "Agence non trouv√©e"
        except:
            return "Agence non trouv√©e"
    
    async def save_apartment(self, apartment_data):
        """Sauvegarde les donn√©es d'un appartement"""
        try:
            os.makedirs('data/appartements', exist_ok=True)
            filename = f"data/appartements/{apartment_data['id']}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(apartment_data, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ Appartement {apartment_data['id']} sauvegard√©")
            
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde: {e}")
    
    async def download_apartment_photos(self, apartment_id, photos):
        """T√©l√©charge les photos d'un appartement localement avec filtrage par taille"""
        try:
            if not photos:
                return
                
            # Cr√©er le dossier pour les photos
            photos_dir = f"data/photos/{apartment_id}"
            os.makedirs(photos_dir, exist_ok=True)
            
            # Supprimer toutes les photos existantes
            if os.path.exists(photos_dir):
                existing_files = [f for f in os.listdir(photos_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
                for existing_file in existing_files:
                    file_path = os.path.join(photos_dir, existing_file)
                    try:
                        os.remove(file_path)
                        print(f"      üóëÔ∏è Photo existante supprim√©e: {existing_file}")
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è Erreur suppression {existing_file}: {e}")
            
            # T√©l√©charger et filtrer les photos
            valid_photos = []
            async with aiohttp.ClientSession() as session:
                for i, photo in enumerate(photos):  # T√©l√©charger toutes les photos disponibles
                    url = photo['url']
                    temp_filename = f"{photos_dir}/temp_photo_{i+1}.jpg"
                    
                    try:
                        async with session.get(url) as response:
                            if response.status == 200:
                                content = await response.read()
                                
                                # Sauvegarder temporairement pour analyser
                                with open(temp_filename, 'wb') as f:
                                    f.write(content)
                                
                                # V√©rifier si c'est une vraie photo d'appartement
                                if self.is_valid_apartment_photo(temp_filename, content):
                                    # Renommer avec format simple: photo1.jpg, photo2.jpg, etc.
                                    photo_number = len(valid_photos) + 1
                                    final_filename = f"{photos_dir}/photo{photo_number}.jpg"
                                    os.rename(temp_filename, final_filename)
                                    valid_photos.append(final_filename)
                                    print(f"      üì∏ Photo {photo_number} t√©l√©charg√©e: {final_filename} ({len(content)} bytes)")
                                else:
                                    # Supprimer la photo invalide
                                    os.remove(temp_filename)
                                    print(f"      ‚ùå Photo {i+1} rejet√©e: {len(content)} bytes (logo/ic√¥ne)")
                            else:
                                print(f"      ‚ùå Erreur photo {i+1}: HTTP {response.status}")
                    except Exception as e:
                        print(f"      ‚ùå Erreur t√©l√©chargement photo {i+1}: {e}")
                        if os.path.exists(temp_filename):
                            os.remove(temp_filename)
            
            print(f"      ‚úÖ {len(valid_photos)} photos d'appartement t√©l√©charg√©es dans {photos_dir}/")
                        
        except Exception as e:
            print(f"‚ùå Erreur t√©l√©chargement photos: {e}")
    
    def is_valid_apartment_photo(self, filepath, content):
        """V√©rifie si une photo est une vraie photo d'appartement"""
        try:
            # V√©rifier la taille du fichier (pas trop petit, pas trop grand)
            if len(content) < 20000 or len(content) > 500000:  # 20KB - 500KB
                return False
            
            # V√©rifier le type de fichier
            if not (content.startswith(b'\xff\xd8\xff') or  # JPEG
                    content.startswith(b'\x89PNG')):  # PNG
                return False
            
            # Pour les PNG, v√©rifier qu'ils ne sont pas des logos (carr√©s petits)
            if content.startswith(b'\x89PNG'):
                # Lire les dimensions PNG
                if len(content) >= 24:
                    width = int.from_bytes(content[16:20], 'big')
                    height = int.from_bytes(content[20:24], 'big')
                    # Rejeter les images carr√©es petites (logos)
                    if width == height and width < 600:
                        return False
                    # Rejeter les images trop petites
                    if width < 400 or height < 300:
                        return False
            
            return True
            
        except Exception as e:
            return False
    
    async def cleanup(self):
        """Ferme le navigateur"""
        if self.browser:
            await self.browser.close()

async def main():
    """Fonction principale"""
    if len(sys.argv) < 2:
        print("Usage: python scrape_jinka.py <URL_ALERTE>")
        print("Exemple: python scrape_jinka.py 'https://www.jinka.fr/alert_result?token=...'")
        return
    
    alert_url = sys.argv[1]
    
    scraper = JinkaScraper()
    
    try:
        await scraper.setup()
        
        if await scraper.login():
            await scraper.scrape_alert_page(alert_url)
            print(f"‚úÖ Scraping termin√©: {len(scraper.apartments)} appartements")
        else:
            print("‚ùå √âchec de la connexion")
    
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
    
    finally:
        await scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
