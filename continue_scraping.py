#!/usr/bin/env python3
"""
Continue le scraping une fois connectÃ© Ã  Jinka
"""

import asyncio
from scrape_jinka import JinkaScraper

async def continue_scraping():
    """Continue le scraping depuis le dashboard connectÃ©"""
    print("ğŸ  CONTINUATION DU SCRAPING JINKA")
    print("=" * 50)
    print("Assure-toi d'Ãªtre connectÃ© Ã  Jinka dans le navigateur")
    print()
    
    scraper = JinkaScraper()
    
    try:
        # 1. Initialiser
        await scraper.setup()
        print("âœ… Scraper initialisÃ©")
        
        # 2. Aller au dashboard
        dashboard_url = "https://www.jinka.fr/asrenter/alert/dashboard/26c2ec3064303aa68ffa43f7c6518733"
        print(f"ğŸŒ Navigation vers: {dashboard_url}")
        
        await scraper.page.goto(dashboard_url)
        await scraper.page.wait_for_timeout(5000)
        
        # 3. VÃ©rifier la connexion
        current_url = scraper.page.url
        print(f"ğŸ“ URL actuelle: {current_url}")
        
        if "sign/in" in current_url:
            print("âŒ Tu n'es pas connectÃ©. Lance d'abord manual_google_auth.py")
            return False
        
        print("âœ… Connexion vÃ©rifiÃ©e")
        
        # 4. Chercher des appartements
        print("\nğŸ” RECHERCHE D'APPARTEMENTS")
        print("-" * 40)
        
        # Essayer diffÃ©rents sÃ©lecteurs
        selectors = [
            'a[href*="ad="]',
            'a[href*="alert_result"]',
            'a.sc-bdVaJa',
            '[data-testid*="apartment"]',
            '.apartment-card',
            '.property-card'
        ]
        
        all_apartments = []
        for selector in selectors:
            try:
                elements = await scraper.page.query_selector_all(selector)
                print(f"   SÃ©lecteur '{selector}': {len(elements)} Ã©lÃ©ments")
                
                for element in elements:
                    href = await element.get_attribute('href')
                    if href and ('ad=' in href or 'alert_result' in href):
                        all_apartments.append(href)
                        
            except Exception as e:
                print(f"   Erreur avec '{selector}': {e}")
        
        # DÃ©dupliquer
        unique_apartments = list(set(all_apartments))
        print(f"\nğŸ  Total d'appartements uniques: {len(unique_apartments)}")
        
        if unique_apartments:
            print("\nğŸ“‹ LISTE DES APPARTEMENTS:")
            for i, apt in enumerate(unique_apartments[:10], 1):
                print(f"   {i}. {apt}")
            
            # 5. Scraper les premiers appartements
            print(f"\nğŸ  SCRAPING DES PREMIERS APPARTEMENTS")
            print("-" * 50)
            
            scraped_count = 0
            for i, apt_url in enumerate(unique_apartments[:5], 1):  # Limiter Ã  5 pour le test
                print(f"\nğŸ  Appartement {i}/{min(5, len(unique_apartments))}")
                print(f"   URL: {apt_url}")
                
                try:
                    # Construire l'URL complÃ¨te si nÃ©cessaire
                    if apt_url.startswith('/'):
                        apt_url = f"https://www.jinka.fr{apt_url}"
                    elif apt_url.startswith('loueragile://'):
                        # Extraire l'ID de l'URL loueragile
                        import re
                        match = re.search(r'ad\?id=(\d+)', apt_url)
                        if match:
                            apt_id = match.group(1)
                            apt_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}"
                    
                    # Scraper l'appartement
                    apartment_data = await scraper.scrape_apartment_details(apt_url)
                    
                    if apartment_data:
                        print(f"   âœ… Appartement scrapÃ©: {apartment_data.get('titre', 'N/A')}")
                        scraped_count += 1
                    else:
                        print(f"   âŒ Ã‰chec du scraping")
                        
                except Exception as e:
                    print(f"   âŒ Erreur: {e}")
            
            print(f"\nğŸ“Š RÃ‰SULTATS:")
            print(f"   Appartements trouvÃ©s: {len(unique_apartments)}")
            print(f"   Appartements scrapÃ©s: {scraped_count}")
            
            if scraped_count > 0:
                print("ğŸ‰ SCRAPING RÃ‰USSI !")
                return True
            else:
                print("âŒ Aucun appartement n'a pu Ãªtre scrapÃ©")
                return False
        else:
            print("âŒ Aucun appartement trouvÃ©")
            return False
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return False
    finally:
        print("\nâš ï¸ Le navigateur restera ouvert")
        print("Ferme-le manuellement quand tu as fini")

if __name__ == "__main__":
    asyncio.run(continue_scraping())
