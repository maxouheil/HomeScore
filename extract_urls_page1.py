#!/usr/bin/env python3
"""
Script pour extraire toutes les URLs d'appartements visibles sur la page 1 du dashboard Jinka
Version simple sans pagination ni scroll - juste pour valider la mÃ©thode d'extraction
"""

import asyncio
import json
import os
import re
from scrape_jinka import JinkaScraper


async def extract_urls_from_page1():
    """
    Extrait toutes les URLs d'appartements visibles sur la page 1 du dashboard
    """
    print("ğŸ” EXTRACTION DES URLs - PAGE 1 DU DASHBOARD")
    print("=" * 60)
    
    scraper = JinkaScraper()
    
    try:
        # 1. Setup
        await scraper.setup()
        print("âœ… Scraper initialisÃ©")
        
        # 2. Aller directement au dashboard (sans login automatique)
        print("\nğŸŒ Navigation directe vers le dashboard...")
        print("   (Si tu n'es pas connectÃ©, connecte-toi manuellement dans le navigateur)")
        
        # URL du dashboard
        dashboard_url = "https://www.jinka.fr/asrenter/alert/dashboard/26c2ec3064303aa68ffa43f7c6518733"
        
        print(f"ğŸ“ Tentative de navigation vers: {dashboard_url}")
        
        # Aller au dashboard directement
        try:
            await scraper.page.goto(dashboard_url, wait_until='networkidle', timeout=30000)
            print("âœ… Navigation rÃ©ussie")
        except Exception as e:
            print(f"âš ï¸ Erreur lors de la navigation: {e}")
            print("   Tentative avec timeout plus long...")
            await scraper.page.goto(dashboard_url, timeout=60000)
        
        await asyncio.sleep(3000)  # Attendre le chargement complet
        
        current_url = scraper.page.url
        print(f"ğŸ“ URL actuelle: {current_url}")
        
        # VÃ©rifier si on est dÃ©jÃ  sur le dashboard
        is_on_dashboard = (
            "dashboard" in current_url.lower() or 
            "asrenter/alert/dashboard" in current_url.lower()
        )
        
        # Si on est redirigÃ© vers login, attendre la connexion manuelle
        if not is_on_dashboard and ("sign/in" in current_url or "auth" in current_url or "couldn't sign" in current_url.lower()):
            print("âš ï¸ Redirection vers login dÃ©tectÃ©e")
            print("   Connecte-toi manuellement dans le navigateur et va sur le dashboard...")
            print("   Le script attendra que tu navigues vers le dashboard...")
            
            # Attendre que l'utilisateur navigue vers le dashboard
            max_wait = 120  # 2 minutes max
            wait_time = 0
            
            while wait_time < max_wait:
                await asyncio.sleep(5)
                current_url = scraper.page.url
                wait_time += 5
                
                print(f"   â³ VÃ©rification ({wait_time}s) - URL: {current_url[:100]}")
                
                # VÃ©rifier plusieurs patterns pour dÃ©tecter le dashboard
                if ("dashboard" in current_url.lower() or 
                    "asrenter/alert/dashboard" in current_url.lower()):
                    print(f"âœ… Dashboard dÃ©tectÃ© aprÃ¨s {wait_time}s!")
                    is_on_dashboard = True
                    break
                elif wait_time % 15 == 0:
                    print(f"   â³ Attente... ({wait_time}s)")
            
            if not is_on_dashboard:
                print("â° Timeout - le dashboard n'a pas Ã©tÃ© atteint")
                print("   Mais on continue quand mÃªme l'extraction...")
        
        # Afficher l'URL finale
        current_url = scraper.page.url
        print(f"\nğŸ“ URL finale: {current_url}")
        
        # Attendre un peu pour s'assurer que la page est complÃ¨tement chargÃ©e
        print("â³ Attente du chargement complet de la page...")
        await asyncio.sleep(5000)  # 5 secondes pour s'assurer que tout est chargÃ©
        
        if is_on_dashboard or "asrenter/alert" in current_url.lower():
            print("âœ… Sur le dashboard - extraction des URLs...")
        else:
            print("âš ï¸ Pas sÃ»r d'Ãªtre sur le dashboard, mais on continue l'extraction...")
        
        # 4. Extraire les URLs avec plusieurs mÃ©thodes
        print("\nğŸ” EXTRACTION DES URLs")
        print("-" * 40)
        print("DÃ©but de l'extraction...")
        
        all_urls = set()  # Utiliser un set pour Ã©viter les doublons
        
        # MÃ©thode 1: SÃ©lecteurs Playwright
        print("\nğŸ“‹ MÃ©thode 1: SÃ©lecteurs Playwright")
        try:
            selectors = [
                'a[href*="alert_result"][href*="ad="]',
                'a[href*="alert_result"]',
                'a[href*="ad="]',
            ]
            
            for selector in selectors:
                try:
                    links = scraper.page.locator(selector)
                    count = await links.count()
                    
                    if count > 0:
                        print(f"   âœ… {count} liens trouvÃ©s avec: {selector}")
                        
                        for i in range(count):
                            try:
                                href = await links.nth(i).get_attribute('href')
                                if href:
                                    # Construire l'URL complÃ¨te si nÃ©cessaire
                                    if href.startswith('/'):
                                        full_url = f"https://www.jinka.fr{href}"
                                    elif href.startswith('http'):
                                        full_url = href
                                    elif href.startswith('loueragile://'):
                                        # Extraire l'ID depuis loueragile://
                                        match = re.search(r'id=(\d+)', href)
                                        if match:
                                            apt_id = match.group(1)
                                            full_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                                        else:
                                            continue
                                    else:
                                        continue
                                    
                                    # VÃ©rifier que c'est bien une URL d'appartement
                                    if 'ad=' in full_url or 'alert_result' in full_url:
                                        all_urls.add(full_url)
                            except Exception as e:
                                continue
                    else:
                        print(f"   âš ï¸  0 liens trouvÃ©s avec: {selector}")
                except Exception as e:
                    print(f"   âŒ Erreur avec sÃ©lecteur {selector}: {e}")
                    continue
        except Exception as e:
            print(f"   âŒ Erreur mÃ©thode sÃ©lecteurs: {e}")
        
        # MÃ©thode 2: Regex sur le HTML brut (backup)
        print("\nğŸ“‹ MÃ©thode 2: Regex sur le HTML")
        try:
            page_content = await scraper.page.content()
            print(f"   ğŸ“„ Taille du HTML: {len(page_content)} caractÃ¨res")
            
            # Patterns pour trouver les URLs d'appartements
            url_patterns = [
                r'href="(/alert_result\?token=[^&]+&ad=\d+[^"]*)"',
                r'href="(https://www\.jinka\.fr/alert_result\?token=[^&]+&ad=\d+[^"]*)"',
            ]
            
            regex_urls_found = 0
            for pattern in url_patterns:
                matches = re.findall(pattern, page_content)
                for match in matches:
                    if match.startswith('/'):
                        full_url = f"https://www.jinka.fr{match}"
                    else:
                        full_url = match
                    
                    if 'ad=' in full_url:
                        all_urls.add(full_url)
                        regex_urls_found += 1
            
            print(f"   âœ… {regex_urls_found} URLs trouvÃ©es avec regex")
        except Exception as e:
            print(f"   âŒ Erreur mÃ©thode regex: {e}")
        
        # MÃ©thode 3: Extraction des IDs depuis loueragile://
        print("\nğŸ“‹ MÃ©thode 3: Extraction depuis loueragile://")
        try:
            page_content = await scraper.page.content()
            
            # Chercher les liens loueragile://
            loueragile_pattern = r'loueragile://[^"]*id=(\d+)'
            matches = re.findall(loueragile_pattern, page_content)
            
            loueragile_ids_found = 0
            for apt_id in matches:
                full_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                all_urls.add(full_url)
                loueragile_ids_found += 1
            
            print(f"   âœ… {loueragile_ids_found} IDs trouvÃ©s depuis loueragile://")
        except Exception as e:
            print(f"   âŒ Erreur mÃ©thode loueragile: {e}")
        
        # Convertir en liste et trier
        unique_urls = sorted(list(all_urls))
        
        print(f"\nğŸ“Š RÃ‰SULTATS")
        print("=" * 60)
        print(f"ğŸ  Total d'URLs uniques trouvÃ©es: {len(unique_urls)}")
        
        if unique_urls:
            # Afficher les premiÃ¨res URLs
            print(f"\nğŸ“‹ PremiÃ¨res URLs trouvÃ©es:")
            for i, url in enumerate(unique_urls[:10], 1):
                print(f"   {i}. {url}")
            
            if len(unique_urls) > 10:
                print(f"   ... et {len(unique_urls) - 10} autres")
            
            # Sauvegarder les URLs
            os.makedirs("data", exist_ok=True)
            urls_file = "data/apartment_urls_page1.json"
            
            with open(urls_file, 'w', encoding='utf-8') as f:
                json.dump(unique_urls, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ URLs sauvegardÃ©es: {urls_file}")
            
            # Prendre un screenshot
            await scraper.page.screenshot(path="data/dashboard_page1_extraction.png")
            print(f"ğŸ“¸ Screenshot: data/dashboard_page1_extraction.png")
            
            return unique_urls
        else:
            print("âŒ Aucune URL trouvÃ©e")
            return []
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        print("\nâš ï¸ Le navigateur restera ouvert")
        print("Ferme-le manuellement quand tu as fini")


async def main():
    """Fonction principale"""
    urls = await extract_urls_from_page1()
    
    if urls:
        print(f"\nğŸ‰ SUCCÃˆS: {len(urls)} URLs d'appartements rÃ©cupÃ©rÃ©es sur la page 1 !")
        print(f"ğŸ“ Fichier: data/apartment_urls_page1.json")
    else:
        print("\nâŒ Aucune URL rÃ©cupÃ©rÃ©e")


if __name__ == "__main__":
    asyncio.run(main())

