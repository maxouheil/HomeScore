#!/usr/bin/env python3
"""
Script qui attend que tu sois connect√© puis lance le scraping
"""

import asyncio
import time
from scrape_jinka import JinkaScraper

async def wait_and_scrape():
    """Attend la connexion puis lance le scraping"""
    print("‚è≥ ATTENTE DE CONNEXION + SCRAPING AUTOMATIQUE")
    print("=" * 60)
    print("1. Connecte-toi manuellement √† Google dans le navigateur")
    print("2. Une fois connect√© √† Jinka, le scraping se lancera automatiquement")
    print()
    
    scraper = JinkaScraper()
    
    try:
        # 1. Initialiser
        await scraper.setup()
        print("‚úÖ Scraper initialis√©")
        
        # 2. Aller √† Jinka
        print("üåê Navigation vers Jinka...")
        await scraper.page.goto("https://www.jinka.fr/sign/in")
        await scraper.page.wait_for_timeout(3000)
        
        # 3. Cliquer sur Google
        print("üîç Recherche du bouton Google...")
        google_button = await scraper.page.query_selector('button:has-text("Continuer avec Google")')
        if not google_button:
            print("‚ùå Bouton Google non trouv√©")
            return False
        
        print("üñ±Ô∏è Clic sur Google...")
        await google_button.click()
        await scraper.page.wait_for_timeout(5000)
        
        # 4. Attendre que tu te connectes manuellement
        print("üîê CONNEXION MANUELLE REQUISE")
        print("=" * 40)
        print("Connecte-toi manuellement √† Google dans le navigateur")
        print("Le script attendra que tu sois connect√© √† Jinka...")
        print()
        
        # Attendre la connexion (v√©rifier toutes les 5 secondes)
        max_wait = 300  # 5 minutes max
        wait_time = 0
        
        while wait_time < max_wait:
            current_url = scraper.page.url
            print(f"‚è≥ Attente... ({wait_time}s) - URL: {current_url[:80]}...")
            
            # V√©rifier si on est connect√© √† Jinka
            if "jinka.fr" in current_url and "sign/in" not in current_url:
                print("üéâ CONNEXION JINKA D√âTECT√âE !")
                break
            
            await scraper.page.wait_for_timeout(5000)
            wait_time += 5
        
        if wait_time >= max_wait:
            print("‚è∞ Timeout - connexion non d√©tect√©e")
            return False
        
        # 5. Aller au dashboard
        print("\nüè† ACC√àS AU DASHBOARD")
        print("-" * 40)
        
        dashboard_url = "https://www.jinka.fr/asrenter/alert/dashboard/26c2ec3064303aa68ffa43f7c6518733"
        await scraper.page.goto(dashboard_url)
        await scraper.page.wait_for_timeout(5000)
        
        dashboard_url_final = scraper.page.url
        print(f"üìç URL dashboard: {dashboard_url_final}")
        
        if "sign/in" in dashboard_url_final:
            print("‚ùå Redirection vers login d√©tect√©e")
            return False
        
        print("‚úÖ Acc√®s au dashboard r√©ussi !")
        
        # 6. Chercher des appartements
        print("\nüîç RECHERCHE D'APPARTEMENTS")
        print("-" * 40)
        
        # Essayer diff√©rents s√©lecteurs
        selectors = [
            'a[href*="ad="]',
            'a[href*="alert_result"]',
            'a.sc-bdVaJa',
            '[data-testid*="apartment"]',
            '.apartment-card',
            '.property-card'
        ]
        
        all_apartments = []
        for selector in selectors:
            try:
                elements = await scraper.page.query_selector_all(selector)
                print(f"   S√©lecteur '{selector}': {len(elements)} √©l√©ments")
                
                for element in elements:
                    href = await element.get_attribute('href')
                    if href and ('ad=' in href or 'alert_result' in href):
                        all_apartments.append(href)
                        
            except Exception as e:
                print(f"   Erreur avec '{selector}': {e}")
        
        # D√©dupliquer
        unique_apartments = list(set(all_apartments))
        print(f"\nüè† Total d'appartements uniques: {len(unique_apartments)}")
        
        if not unique_apartments:
            print("‚ùå Aucun appartement trouv√©")
            return False
        
        print("\nüìã LISTE DES APPARTEMENTS:")
        for i, apt in enumerate(unique_apartments[:10], 1):
            print(f"   {i}. {apt}")
        
        # 7. Scraper les appartements
        print(f"\nüè† SCRAPING DE {min(10, len(unique_apartments))} APPARTEMENTS")
        print("-" * 50)
        
        scraped_count = 0
        for i, apt_url in enumerate(unique_apartments[:10], 1):  # Limiter √† 10 pour le test
            print(f"\nüè† Appartement {i}/{min(10, len(unique_apartments))}")
            print(f"   URL: {apt_url}")
            
            try:
                # Construire l'URL compl√®te si n√©cessaire
                if apt_url.startswith('/'):
                    apt_url = f"https://www.jinka.fr{apt_url}"
                elif apt_url.startswith('loueragile://'):
                    # Extraire l'ID de l'URL loueragile
                    import re
                    match = re.search(r'ad\?id=(\d+)', apt_url)
                    if match:
                        apt_id = match.group(1)
                        apt_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}"
                
                # Scraper l'appartement
                apartment_data = await scraper.scrape_apartment_details(apt_url)
                
                if apartment_data:
                    print(f"   ‚úÖ Appartement scrap√©: {apartment_data.get('titre', 'N/A')}")
                    scraped_count += 1
                else:
                    print(f"   ‚ùå √âchec du scraping")
                    
            except Exception as e:
                print(f"   ‚ùå Erreur: {e}")
        
        print(f"\nüìä R√âSULTATS FINAUX:")
        print(f"   Appartements trouv√©s: {len(unique_apartments)}")
        print(f"   Appartements scrap√©s: {scraped_count}")
        
        if scraped_count > 0:
            print("üéâ SCRAPING R√âUSSI !")
            return True
        else:
            print("‚ùå Aucun appartement n'a pu √™tre scrap√©")
            return False
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False
    finally:
        print("\n‚ö†Ô∏è Le navigateur restera ouvert")
        print("Ferme-le manuellement quand tu as fini")

if __name__ == "__main__":
    asyncio.run(wait_and_scrape())
