# Guide : R√©cup√©ration des Nouveaux Appartements Jinka

## üìã Vue d'ensemble

Ce document d√©crit la m√©thode qui fonctionne pour r√©cup√©rer et scraper les nouveaux appartements depuis le dashboard Jinka, en √©vitant les probl√®mes de connexion et de d√©tection des liens.

## üéØ Probl√®mes r√©solus

### 1. **Probl√®me de connexion automatique**
- **Avant** : Le script essayait de se connecter automatiquement avec les identifiants depuis `.env`, mais √©chouait souvent
- **Probl√®me** : La connexion automatique ne fonctionnait pas de mani√®re fiable, surtout avec l'authentification Google/2FA
- **Solution** : Passage √† une connexion **100% manuelle** avec d√©tection automatique de la connexion

### 2. **Rafra√Æchissement continu de la page**
- **Avant** : Le script rafra√Æchissait la page en boucle pour v√©rifier la connexion, emp√™chant l'utilisateur de se connecter
- **Probl√®me** : `page.goto()` √©tait appel√© dans une boucle, ce qui relan√ßait la page de cookies √† chaque fois
- **Solution** : V√©rification de l'URL **SANS navigation** pendant l'attente de connexion

### 3. **Page de cookies qui s'ouvre en boucle**
- **Avant** : Navigation r√©p√©t√©e vers le dashboard d√©clenchait les popups de cookies
- **Probl√®me** : `page.goto(dashboard_url)` √©tait appel√© plusieurs fois
- **Solution** : Navigation vers le dashboard **UNE SEULE FOIS** apr√®s d√©tection de la connexion, avec fermeture automatique des popups

## üîß Changements techniques principaux

### M√©thode de connexion

#### **AVANT** (ne fonctionnait pas)
```python
# Tentative de connexion automatique
login_success = await scraper.login()

# V√©rification en naviguant vers le dashboard
await scraper.page.goto(dashboard_url)  # ‚ùå D√©clenche les cookies
if "sign/in" in current_url:
    # Boucle qui rafra√Æchit la page...
    while True:
        await scraper.page.goto(dashboard_url)  # ‚ùå Rafra√Æchit en boucle
        current_url = scraper.page.url
        # ...
```

#### **APR√àS** (fonctionne)
```python
# 1. Aller directement √† la page de login
await scraper.page.goto('https://www.jinka.fr/sign/in')
await scraper.page.wait_for_load_state('networkidle')

# 2. Cliquer sur Google pour faciliter (optionnel)
google_button = scraper.page.locator('button:has-text("Continuer avec Google")').first
if await google_button.count() > 0:
    await google_button.click()

# 3. Attendre la connexion SANS rafra√Æchir
while wait_time < max_wait:
    # ‚úÖ V√©rifier l'URL SANS changer de page
    current_url = scraper.page.url  # Pas de page.goto() !
    
    if "jinka.fr" in current_url and "sign/in" not in current_url:
        login_success = True
        break
    
    await asyncio.sleep(2)  # Attendre 2 secondes

# 4. Aller au dashboard UNE SEULE FOIS apr√®s connexion
await scraper.page.goto(dashboard_url)
# Fermer les popups de cookies
```

### Points cl√©s de la solution

1. **Pas de navigation pendant l'attente**
   - Utiliser `page.url` pour v√©rifier l'URL actuelle
   - Ne JAMAIS appeler `page.goto()` dans la boucle d'attente
   - Laisser l'utilisateur compl√©ter sa connexion tranquillement

2. **Navigation unique vers le dashboard**
   - Aller au dashboard **UNE SEULE FOIS** apr√®s d√©tection de la connexion
   - √âviter les multiples navigations qui d√©clenchent les popups

3. **Fermeture automatique des popups de cookies**
   - Apr√®s la navigation unique, chercher et fermer les popups de cookies
   - Utiliser plusieurs s√©lecteurs pour √™tre robuste :
     ```python
     cookie_selectors = [
         'button:has-text("Accepter")',
         'button:has-text("Accept")',
         '[id*="cookie"] button',
         '[class*="cookie"] button',
         '.cookie-consent button',
         '#cookieConsent button'
     ]
     ```

4. **D√©tection de connexion robuste**
   - V√©rifier que l'URL contient `jinka.fr`
   - V√©rifier que l'URL ne contient PAS `sign/in`
   - V√©rifier que l'URL ne contient PAS `accounts.google.com` (on n'est plus sur Google)

## üìù Fonctionnement du script complet

### √âtape 1 : Initialisation
```python
scraper = JinkaScraper()
await scraper.setup()  # Ouvre Chrome en mode visible
```

### √âtape 2 : Page de login
```python
await scraper.page.goto('https://www.jinka.fr/sign/in')
# Clique sur "Continuer avec Google" pour faciliter
```

### √âtape 3 : Attente de connexion manuelle
```python
# Boucle qui v√©rifie l'URL SANS rafra√Æchir
while wait_time < max_wait:
    current_url = scraper.page.url  # ‚úÖ Pas de navigation
    if est_connecte(current_url):
        break
    await asyncio.sleep(2)
```

### √âtape 4 : Navigation vers le dashboard
```python
# UNE SEULE FOIS apr√®s connexion d√©tect√©e
await scraper.page.goto(dashboard_url)
# Fermer les popups de cookies
```

### √âtape 5 : Extraction des URLs
```python
# Scroll pour charger tous les appartements (lazy loading)
await scroll_to_load_all_apartments(scraper.page)

# Cliquer sur "Voir plus" si disponible
await click_load_more_until_done(scraper.page)

# Extraire toutes les URLs
all_urls = await extract_urls_from_page(scraper.page)
```

### √âtape 6 : Filtrage des nouveaux appartements
```python
# Charger les IDs d√©j√† scrap√©s
existing_ids = load_existing_apartment_ids()

# Filtrer pour ne garder que les nouveaux
new_urls = filter_new_apartments(all_urls, existing_ids)
```

### √âtape 7 : Scraping des nouveaux appartements
```python
for url in new_urls:
    apartment_data = await scraper.scrape_apartment(url)
    # Sauvegarde automatique dans data/appartements/
```

## üîç M√©thode d'extraction des URLs

La fonction `extract_urls_from_page()` utilise **3 m√©thodes combin√©es** pour √™tre robuste :

### M√©thode 1 : S√©lecteurs Playwright
```python
links = page.locator('a[href*="alert_result"][href*="ad="], a[href*="ad="]')
count = await links.count()
for i in range(count):
    href = await links.nth(i).get_attribute('href')
    # Construire l'URL compl√®te si n√©cessaire
```

### M√©thode 2 : Regex sur le HTML
```python
page_content = await page.content()
url_patterns = [
    r'href="(/alert_result\?token=[^&]+&ad=\d+[^"]*)"',
    r'href="(https://www\.jinka\.fr/alert_result\?token=[^&]+&ad=\d+[^"]*)"',
    r'ad=(\d+)',
]
```

### M√©thode 3 : JavaScript injection
```javascript
const links = Array.from(document.querySelectorAll('a[href*="ad="]'));
return links.map(link => {
    let href = link.href || link.getAttribute('href');
    if (href && !href.startsWith('http') && href.startsWith('/')) {
        href = 'https://www.jinka.fr' + href;
    }
    return href;
}).filter(href => href && href.includes('alert_result') && href.includes('ad='));
```

## ‚úÖ Checklist pour que √ßa fonctionne

- [x] Connexion 100% manuelle (pas d'authentification automatique)
- [x] Pas de `page.goto()` dans la boucle d'attente
- [x] Navigation vers le dashboard UNE SEULE FOIS apr√®s connexion
- [x] Fermeture automatique des popups de cookies
- [x] Scroll infini pour charger tous les appartements
- [x] Cliquer sur "Voir plus" si disponible
- [x] Extraction des URLs avec 3 m√©thodes combin√©es
- [x] Filtrage pour ne scraper que les nouveaux appartements

## üöÄ Utilisation

```bash
python scrape_new_apartments.py
```

Le script va :
1. Ouvrir Chrome
2. Aller √† la page de login Jinka
3. Cliquer sur "Continuer avec Google"
4. **Attendre que tu te connectes manuellement** (sans rafra√Æchir)
5. D√©tecter automatiquement la connexion
6. Aller au dashboard une seule fois
7. Fermer les popups de cookies
8. Extraire tous les nouveaux appartements
9. Les scraper automatiquement

## üìä R√©sultats

Les nouveaux appartements sont sauvegard√©s dans :
- `data/appartements/{id}.json` - Donn√©es compl√®tes de l'appartement
- `data/photos/{id}/` - Photos t√©l√©charg√©es localement

## üîÑ Comparaison avec l'ancienne m√©thode

| Aspect | Ancienne m√©thode | Nouvelle m√©thode |
|--------|------------------|------------------|
| Connexion | Automatique (√©chouait souvent) | Manuelle (100% fiable) |
| Rafra√Æchissement | En boucle (bloquait la connexion) | Aucun (attente passive) |
| Navigation dashboard | Plusieurs fois (popups cookies) | Une seule fois |
| Popups cookies | Non g√©r√©es | Fermeture automatique |
| Extraction URLs | 1 m√©thode | 3 m√©thodes combin√©es |
| D√©tection connexion | Tentative de navigation | V√©rification URL sans navigation |

## üí° Le√ßons apprises

1. **Ne jamais rafra√Æchir pendant une action utilisateur** : Si l'utilisateur doit faire quelque chose manuellement, ne pas interf√©rer avec la page

2. **Une seule navigation apr√®s connexion** : √âviter les multiples navigations qui d√©clenchent des popups

3. **V√©rification passive de l'URL** : Utiliser `page.url` au lieu de `page.goto()` pour v√©rifier l'√©tat

4. **Gestion des popups** : Toujours pr√©voir de fermer les popups apr√®s une navigation

5. **M√©thodes multiples pour extraction** : Combiner plusieurs m√©thodes (s√©lecteurs, regex, JS) pour √™tre robuste

## üìÖ Date de cr√©ation

2024-11-01 - Documentation des changements qui ont permis au scraping de fonctionner correctement






