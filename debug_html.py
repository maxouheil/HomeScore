#!/usr/bin/env python3
"""
Script pour analyser le HTML et trouver le bon format des URLs
"""

import asyncio
import re
from scrape_jinka import JinkaScraper

async def debug_html():
    """Analyse le HTML pour trouver les URLs"""
    print("ğŸ” ANALYSE DU HTML POUR TROUVER LES URLs")
    print("=" * 60)
    
    scraper = JinkaScraper()
    
    try:
        await scraper.setup()
        print("âœ… Chrome ouvert")
        
        # Aller au dashboard
        dashboard_url = "https://www.jinka.fr/asrenter/alert/dashboard/26c2ec3064303aa68ffa43f7c6518733"
        await scraper.page.goto(dashboard_url)
        await scraper.page.wait_for_timeout(5000)
        
        current_url = scraper.page.url
        print(f"ğŸ“ URL actuelle: {current_url}")
        
        if "sign/in" in current_url:
            print("âŒ Redirection vers login")
            print("ğŸ’¡ Connecte-toi manuellement dans le navigateur")
            print("â³ J'attends que tu te connectes...")
            
            # Attendre la connexion
            max_wait = 300
            wait_time = 0
            
            while wait_time < max_wait:
                await scraper.page.wait_for_timeout(5000)
                current_url = scraper.page.url
                print(f"â³ Attente... ({wait_time}s)")
                
                if "jinka.fr" in current_url and "sign/in" not in current_url:
                    print("ğŸ‰ CONNEXION DÃ‰TECTÃ‰E !")
                    break
                
                wait_time += 5
            
            if wait_time >= max_wait:
                print("â° Timeout")
                return False
        
        print("âœ… AccÃ¨s au dashboard rÃ©ussi !")
        
        # Extraire le HTML
        page_content = await scraper.page.content()
        print(f"ğŸ“„ Taille du HTML: {len(page_content)} caractÃ¨res")
        
        # Chercher tous les liens href
        print("\nğŸ” RECHERCHE DE TOUS LES LIENS HREF...")
        href_pattern = r'href="([^"]*)"'
        all_hrefs = re.findall(href_pattern, page_content)
        
        print(f"ğŸ“‹ Total de liens href trouvÃ©s: {len(all_hrefs)}")
        
        # Filtrer les liens d'appartements
        apartment_links = []
        for href in all_hrefs:
            if 'alert_result' in href or 'ad=' in href:
                apartment_links.append(href)
        
        print(f"ğŸ  Liens d'appartements trouvÃ©s: {len(apartment_links)}")
        
        if apartment_links:
            print("\nğŸ“‹ LIENS D'APPARTEMENTS:")
            for i, link in enumerate(apartment_links[:20], 1):  # Afficher les 20 premiers
                print(f"   {i}. {link}")
            
            # Chercher les IDs d'appartements
            print("\nğŸ” RECHERCHE DES IDs D'APPARTEMENTS...")
            id_pattern = r'ad=(\d+)'
            apartment_ids = re.findall(id_pattern, page_content)
            unique_ids = list(set(apartment_ids))
            
            print(f"ğŸ  IDs d'appartements trouvÃ©s: {len(unique_ids)}")
            for i, apt_id in enumerate(unique_ids[:10], 1):
                print(f"   {i}. ID: {apt_id}")
            
            # Construire les URLs complÃ¨tes
            print("\nğŸ”— CONSTRUCTION DES URLs COMPLÃˆTES...")
            base_url = "https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
            
            full_urls = []
            for apt_id in unique_ids:
                full_url = base_url.format(apt_id)
                full_urls.append(full_url)
            
            print(f"âœ… {len(full_urls)} URLs complÃ¨tes construites")
            
            # Sauvegarder
            import json
            import os
            os.makedirs("data", exist_ok=True)
            
            with open("data/all_apartment_urls.json", 'w', encoding='utf-8') as f:
                json.dump(full_urls, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ URLs sauvegardÃ©es: data/all_apartment_urls.json")
            
            # Prendre un screenshot
            await scraper.page.screenshot(path="data/dashboard_debug.png")
            print(f"ğŸ“¸ Screenshot: data/dashboard_debug.png")
            
            return full_urls
        else:
            print("âŒ Aucun lien d'appartement trouvÃ©")
            return False
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return False
    finally:
        print("\nâš ï¸ Le navigateur restera ouvert")
        print("Ferme-le manuellement quand tu as fini")

if __name__ == "__main__":
    asyncio.run(debug_html())
