#!/usr/bin/env python3
"""
Version ultra-simple: extrait les URLs MAINTENANT depuis la page ouverte
"""

import asyncio
import json
import re
import os
from scrape_jinka import JinkaScraper

async def extract_now():
    scraper = JinkaScraper()
    
    try:
        await scraper.setup()
        print("âœ… Scraper initialisÃ©")
        print("âš ï¸ Assure-toi d'Ãªtre sur le dashboard dans le navigateur qui va s'ouvrir")
        print("   Attente de 5 secondes...")
        await asyncio.sleep(5)
        
        # Lire l'URL actuelle (tu es dÃ©jÃ  sur le dashboard)
        current_url = scraper.page.url
        print(f"ğŸ“ URL actuelle dans le navigateur: {current_url}")
        
        # Si ce n'est pas le dashboard, aller dessus
        if "dashboard" not in current_url.lower():
            dashboard_url = "https://www.jinka.fr/asrenter/alert/dashboard/26c2ec3064303aa68ffa43f7c6518733"
            print(f"ğŸŒ Navigation vers le dashboard...")
            await scraper.page.goto(dashboard_url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(3000)
            current_url = scraper.page.url
            print(f"ğŸ“ URL aprÃ¨s navigation: {current_url}")
        
        print("\nğŸ” EXTRACTION EN COURS...")
        print("-" * 50)
        
        # MÃ©thode 1: SÃ©lecteurs
        print("\n1ï¸âƒ£ MÃ©thode sÃ©lecteurs Playwright...")
        urls_set = set()
        
        try:
            # Chercher tous les liens avec ad=
            links = scraper.page.locator('a[href*="ad="]')
            count = await links.count()
            print(f"   âœ… {count} liens trouvÃ©s")
            
            for i in range(count):
                try:
                    href = await links.nth(i).get_attribute('href')
                    if href:
                        if href.startswith('/'):
                            full_url = f"https://www.jinka.fr{href}"
                        elif href.startswith('http'):
                            full_url = href
                        elif 'loueragile://' in href:
                            match = re.search(r'id=(\d+)', href)
                            if match:
                                apt_id = match.group(1)
                                full_url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                            else:
                                continue
                        else:
                            continue
                        
                        if 'ad=' in full_url:
                            urls_set.add(full_url)
                            if i < 5:  # Afficher les 5 premiers
                                print(f"      {i+1}. {full_url[:80]}...")
                except:
                    continue
        except Exception as e:
            print(f"   âŒ Erreur: {e}")
        
        # MÃ©thode 2: Regex sur HTML
        print(f"\n2ï¸âƒ£ MÃ©thode regex sur HTML...")
        try:
            page_content = await scraper.page.content()
            # Chercher tous les IDs d'appartements
            ids_found = re.findall(r'ad=(\d+)', page_content)
            unique_ids = list(set(ids_found))
            print(f"   âœ… {len(unique_ids)} IDs uniques trouvÃ©s")
            
            for apt_id in unique_ids:
                url = f"https://www.jinka.fr/alert_result?token=26c2ec3064303aa68ffa43f7c6518733&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                urls_set.add(url)
        except Exception as e:
            print(f"   âŒ Erreur: {e}")
        
        # RÃ©sultats finaux
        all_urls = sorted(list(urls_set))
        
        print(f"\n{'='*50}")
        print(f"ğŸ“Š RÃ‰SULTATS FINAUX")
        print(f"{'='*50}")
        print(f"ğŸ  Total: {len(all_urls)} URLs uniques trouvÃ©es")
        
        if all_urls:
            print(f"\nğŸ“‹ Liste complÃ¨te:")
            for i, url in enumerate(all_urls, 1):
                print(f"   {i}. {url}")
            
            # Sauvegarder
            os.makedirs("data", exist_ok=True)
            output_file = "data/apartment_urls_page1.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_urls, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ URLs sauvegardÃ©es dans: {output_file}")
            print(f"âœ… TERMINÃ‰!")
        else:
            print(f"\nâŒ Aucune URL trouvÃ©e")
            print(f"   VÃ©rifie que tu es bien sur le dashboard avec des appartements visibles")
        
        return all_urls
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        print("\nâš ï¸ Le navigateur restera ouvert - ferme-le manuellement")

if __name__ == "__main__":
    asyncio.run(extract_now())

