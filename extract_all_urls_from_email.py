#!/usr/bin/env python3
"""
Script principal pour extraire TOUTES les URLs d'appartements depuis les emails Jinka
G√®re l'historique et la d√©duplication pour √©viter les doublons
"""

import imaplib
import email
import re
import json
import os
from datetime import datetime, timedelta
from email.header import decode_header
from dotenv import load_dotenv
from html import unescape
from bs4 import BeautifulSoup

load_dotenv()

# Token Jinka connu
JINKA_TOKEN = "26c2ec3064303aa68ffa43f7c6518733"

def decode_mime_words(s):
    """D√©code les en-t√™tes MIME"""
    if not s:
        return ""
    decoded = decode_header(s)
    return ''.join([text.decode(encoding or 'utf-8') if isinstance(text, bytes) else text 
                    for text, encoding in decoded])

def extract_apartment_id_from_url(url):
    """Extrait l'ID d'appartement depuis une URL"""
    match = re.search(r'ad=(\d+)', url)
    return match.group(1) if match else None

def normalize_url(url):
    """
    Normalise une URL pour la d√©duplication
    Construit l'URL compl√®te si n√©cessaire
    """
    if not url:
        return None
    
    # Si c'est juste un ID num√©rique
    if url.isdigit():
        return f"https://www.jinka.fr/alert_result?token={JINKA_TOKEN}&ad={url}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
    
    # Si c'est un lien relatif
    if url.startswith('/'):
        return f"https://www.jinka.fr{url}"
    
    # Si c'est d√©j√† une URL compl√®te
    if url.startswith('http'):
        return url
    
    # Si c'est un format loueragile
    if 'loueragile://' in url:
        match = re.search(r'id=(\d+)', url)
        if match:
            apt_id = match.group(1)
            return f"https://www.jinka.fr/alert_result?token={JINKA_TOKEN}&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
    
    return None

def extract_urls_from_email_body(body):
    """
    Extrait les URLs d'appartements depuis le corps de l'email
    G√®re HTML et texte brut
    """
    urls = set()
    
    if not body:
        return urls
    
    # D√©coder les entit√©s HTML
    body = unescape(body)
    
    # Si c'est du HTML, extraire le texte aussi
    html_text = ""
    if '<html' in body.lower() or '<body' in body.lower():
        try:
            soup = BeautifulSoup(body, 'html.parser')
            html_text = soup.get_text()
            # Extraire aussi les liens href
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if href:
                    normalized = normalize_url(href)
                    if normalized and 'ad=' in normalized:
                        urls.add(normalized)
        except:
            pass
    
    # Patterns pour trouver les URLs d'appartements Jinka
    patterns = [
        # URLs compl√®tes
        r'https://www\.jinka\.fr/alert_result\?token=[^&\s<>"]+&ad=\d+[^\s<>"]*',
        r'https://www\.jinka\.fr/alert_result\?token=[^&\s<>"]+&ad=\d+',
        # Liens relatifs dans href
        r'href=["\'](/alert_result\?token=[^"\']+&ad=\d+[^"\']*)["\']',
        r'href=["\'](https://www\.jinka\.fr/alert_result[^"\']+)["\']',
        # IDs seuls (fallback)
        r'ad=(\d{6,})',  # Au moins 6 chiffres pour √©viter les faux positifs
        # Format loueragile
        r'loueragile://[^"\s<>]*id=(\d+)',
    ]
    
    # Chercher dans le body et le texte HTML
    texts_to_search = [body]
    if html_text:
        texts_to_search.append(html_text)
    
    for text in texts_to_search:
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match else None
                
                if match:
                    normalized = normalize_url(match)
                    if normalized and ('ad=' in normalized or 'alert_result' in normalized):
                        # V√©rifier que c'est bien une URL valide
                        apt_id = extract_apartment_id_from_url(normalized)
                        if apt_id:
                            urls.add(normalized)
    
    return urls

def load_url_history():
    """Charge l'historique des URLs d√©j√† extraites"""
    history_file = "data/apartment_urls_history.json"
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, dict):
                    return data.get('urls', []), data.get('last_extraction', None)
                elif isinstance(data, list):
                    return data, None
        except:
            pass
    return [], None

def save_url_history(urls, extraction_date=None):
    """Sauvegarde l'historique des URLs"""
    os.makedirs("data", exist_ok=True)
    history_file = "data/apartment_urls_history.json"
    
    data = {
        'urls': urls,
        'last_extraction': extraction_date or datetime.now().isoformat(),
        'total_count': len(urls)
    }
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ Historique sauvegard√©: {len(urls)} URLs dans {history_file}")

def get_jinka_emails(imap_server='imap.gmail.com', email_address=None, password=None, 
                     days_back=90, sender_filters=None):
    """
    R√©cup√®re les emails d'alerte Jinka depuis une bo√Æte email
    
    Args:
        imap_server: Serveur IMAP (par d√©faut Gmail)
        email_address: Adresse email
        password: Mot de passe ou app password
        days_back: Nombre de jours en arri√®re pour chercher
        sender_filters: Liste de filtres pour l'exp√©diteur
    """
    if sender_filters is None:
        sender_filters = ['jinka', 'noreply@jinka.fr', 'alertes@jinka.fr']
    
    if not email_address:
        email_address = os.getenv('JINKA_EMAIL') or os.getenv('EMAIL')
    
    if not password:
        password = os.getenv('EMAIL_PASSWORD') or os.getenv('GMAIL_APP_PASSWORD')
    
    if not email_address or not password:
        print("‚ùå Email ou mot de passe non fourni")
        print("   Configure JINKA_EMAIL et EMAIL_PASSWORD dans .env")
        return []
    
    print(f"üìß Connexion √† {email_address}...")
    
    try:
        # Connexion IMAP
        mail = imaplib.IMAP4_SSL(imap_server)
        mail.login(email_address, password)
        print("‚úÖ Connexion IMAP r√©ussie")
        
        # S√©lectionner la bo√Æte de r√©ception
        mail.select('inbox')
        print("‚úÖ Bo√Æte de r√©ception s√©lectionn√©e")
        
        # Construire la requ√™te de recherche
        date_since = (datetime.now() - timedelta(days=days_back)).strftime('%d-%b-%Y')
        
        # Chercher les emails de Jinka - simplifier la requ√™te
        # Gmail IMAP a des limitations, on va chercher par FROM d'abord
        all_email_ids = set()
        
        print(f"üîç Recherche d'emails depuis {days_back} jours...")
        print(f"   Filtres: {', '.join(sender_filters)}")
        
        # Chercher pour chaque exp√©diteur s√©par√©ment
        for sender in sender_filters:
            try:
                search_query = f'(SINCE {date_since}) FROM "{sender}"'
                status, messages = mail.search(None, search_query)
                if status == 'OK' and messages[0]:
                    email_ids = messages[0].split()
                    all_email_ids.update(email_ids)
                    print(f"   ‚úÖ {len(email_ids)} emails trouv√©s avec filtre: {sender}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è Erreur avec filtre {sender}: {e}")
                continue
        
        # Chercher aussi par sujet
        try:
            search_query = f'(SINCE {date_since}) SUBJECT "jinka"'
            status, messages = mail.search(None, search_query)
            if status == 'OK' and messages[0]:
                email_ids = messages[0].split()
                all_email_ids.update(email_ids)
                print(f"   ‚úÖ {len(email_ids)} emails trouv√©s avec sujet 'jinka'")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur recherche par sujet: {e}")
        
        email_ids = list(all_email_ids)
        
        if status != 'OK':
            print("‚ùå Erreur lors de la recherche")
            return []
        
        # email_ids est d√©j√† d√©fini ci-dessus
        print(f"üì¨ {len(email_ids)} emails uniques trouv√©s au total")
        
        all_urls = set()
        processed_emails = 0
        
        # Parcourir les emails
        for i, email_id in enumerate(email_ids, 1):
            try:
                # R√©cup√©rer l'email
                status, msg_data = mail.fetch(email_id, '(RFC822)')
                
                if status != 'OK':
                    continue
                
                # Parser l'email
                email_body = msg_data[0][1]
                email_message = email.message_from_bytes(email_body)
                
                # Obtenir le sujet et l'exp√©diteur
                subject = decode_mime_words(email_message['Subject'] or '')
                sender = decode_mime_words(email_message['From'] or '')
                date = email_message['Date']
                
                if i <= 5 or i % 10 == 0:  # Afficher les 5 premiers et tous les 10
                    print(f"\nüìß Email {i}/{len(email_ids)}: {subject[:50]}...")
                    print(f"   De: {sender[:50]}...")
                
                # Extraire le corps de l'email (HTML et texte)
                body_html = ""
                body_text = ""
                
                if email_message.is_multipart():
                    for part in email_message.walk():
                        content_type = part.get_content_type()
                        content_disposition = str(part.get("Content-Disposition", ""))
                        
                        # Ignorer les pi√®ces jointes
                        if "attachment" in content_disposition:
                            continue
                        
                        if content_type == "text/html":
                            try:
                                body_html += part.get_payload(decode=True).decode('utf-8', errors='ignore')
                            except:
                                pass
                        elif content_type == "text/plain":
                            try:
                                body_text += part.get_payload(decode=True).decode('utf-8', errors='ignore')
                            except:
                                pass
                else:
                    try:
                        payload = email_message.get_payload(decode=True)
                        if payload:
                            decoded = payload.decode('utf-8', errors='ignore')
                            if '<html' in decoded.lower():
                                body_html = decoded
                            else:
                                body_text = decoded
                    except:
                        pass
                
                # Extraire les URLs depuis HTML et texte
                urls_from_email = set()
                if body_html:
                    urls_from_email.update(extract_urls_from_email_body(body_html))
                if body_text:
                    urls_from_email.update(extract_urls_from_email_body(body_text))
                
                if urls_from_email:
                    print(f"   ‚úÖ {len(urls_from_email)} URLs trouv√©es")
                    all_urls.update(urls_from_email)
                elif i <= 3:  # Debug pour les 3 premiers emails
                    print(f"   üîç Debug: HTML={len(body_html)} chars, Text={len(body_text)} chars")
                    # Chercher manuellement quelques patterns pour debug
                    test_body = body_html + body_text
                    if 'alert_result' in test_body:
                        print(f"      ‚ö†Ô∏è 'alert_result' trouv√© dans le body")
                        # Essayer de trouver directement
                        direct_matches = re.findall(r'https?://[^"\s<>]+alert_result[^"\s<>]+', test_body)
                        if direct_matches:
                            print(f"      ‚úÖ URLs directes trouv√©es: {len(direct_matches)}")
                            for url in direct_matches[:2]:
                                print(f"         {url[:80]}...")
                    if 'ad=' in test_body:
                        matches = re.findall(r'ad=(\d{6,})', test_body)
                        if matches:
                            print(f"      ‚ö†Ô∏è IDs trouv√©s avec regex: {matches[:3]}")
                            # Essayer de construire les URLs
                            for apt_id in matches[:2]:
                                test_url = f"https://www.jinka.fr/alert_result?token={JINKA_TOKEN}&ad={apt_id}&from=dashboard_card&from_alert_filter=all&from_alert_page=1"
                                print(f"         URL construite: {test_url[:80]}...")
                
                processed_emails += 1
                
            except Exception as e:
                if i <= 5:  # Afficher les erreurs pour les premiers emails
                    print(f"   ‚ö†Ô∏è Erreur lors du traitement de l'email {i}: {e}")
                continue
        
        mail.close()
        mail.logout()
        
        print(f"\n‚úÖ {processed_emails} emails trait√©s")
        return sorted(list(all_urls))
        
    except imaplib.IMAP4.error as e:
        print(f"‚ùå Erreur IMAP: {e}")
        print("   V√©rifie que tu utilises un 'App Password' pour Gmail")
        return []
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return []

def main():
    """Fonction principale"""
    print("=" * 70)
    print("üìß EXTRACTION COMPL√àTE DES URLs DEPUIS LES EMAILS JINKA")
    print("=" * 70)
    print()
    
    # Charger l'historique
    print("üìö Chargement de l'historique...")
    history_urls, last_extraction = load_url_history()
    history_set = set(history_urls)
    print(f"   Historique: {len(history_urls)} URLs d√©j√† extraites")
    if last_extraction:
        print(f"   Derni√®re extraction: {last_extraction}")
    
    # Configuration
    email_address = os.getenv('JINKA_EMAIL') or os.getenv('EMAIL')
    password = os.getenv('EMAIL_PASSWORD') or os.getenv('GMAIL_APP_PASSWORD')
    
    if not email_address or not password:
        print("\n‚ö†Ô∏è Configuration email manquante")
        print("   Ajoute dans .env:")
        print("   JINKA_EMAIL=ton_email@gmail.com")
        print("   EMAIL_PASSWORD=ton_app_password")
        print("\n   Pour cr√©er un App Password Gmail:")
        print("   https://myaccount.google.com/apppasswords")
        return []
    
    # R√©cup√©rer les emails (90 derniers jours)
    print("\n" + "=" * 70)
    urls_from_emails = get_jinka_emails(
        email_address=email_address,
        password=password,
        days_back=90,  # Chercher dans les 90 derniers jours
        sender_filters=['jinka', 'noreply@jinka.fr', 'alertes@jinka.fr']
    )
    
    if not urls_from_emails:
        print("\n‚ùå Aucune URL trouv√©e dans les emails")
        return []
    
    print("\n" + "=" * 70)
    print(f"üìä R√âSULTATS DE L'EXTRACTION")
    print("=" * 70)
    print(f"üè† URLs trouv√©es dans les emails: {len(urls_from_emails)}")
    
    # D√©dupliquer avec l'historique
    new_urls = []
    for url in urls_from_emails:
        apt_id = extract_apartment_id_from_url(url)
        # V√©rifier si cette URL ou cet ID existe d√©j√† dans l'historique
        is_new = True
        if url in history_set:
            is_new = False
        elif apt_id:
            # V√©rifier par ID
            for hist_url in history_urls:
                hist_id = extract_apartment_id_from_url(hist_url)
                if hist_id == apt_id:
                    is_new = False
                    break
        
        if is_new:
            new_urls.append(url)
    
    # Combiner historique et nouvelles URLs
    all_urls = sorted(list(set(history_urls + urls_from_emails)))
    
    print(f"üÜï Nouvelles URLs: {len(new_urls)}")
    print(f"üìö Total URLs (historique + nouvelles): {len(all_urls)}")
    
    # Sauvegarder
    os.makedirs("data", exist_ok=True)
    
    # Sauvegarder toutes les URLs
    all_urls_file = "data/all_apartment_urls_from_email.json"
    with open(all_urls_file, 'w', encoding='utf-8') as f:
        json.dump(all_urls, f, indent=2, ensure_ascii=False)
    print(f"\nüíæ Toutes les URLs sauvegard√©es: {all_urls_file}")
    
    # Sauvegarder l'historique mis √† jour
    save_url_history(all_urls)
    
    # Sauvegarder seulement les nouvelles URLs
    if new_urls:
        new_urls_file = "data/new_apartment_urls_from_email.json"
        with open(new_urls_file, 'w', encoding='utf-8') as f:
            json.dump(new_urls, f, indent=2, ensure_ascii=False)
        print(f"üíæ Nouvelles URLs sauvegard√©es: {new_urls_file}")
        
        print(f"\nüìã Nouvelles URLs trouv√©es:")
        for i, url in enumerate(new_urls[:10], 1):
            apt_id = extract_apartment_id_from_url(url)
            print(f"   {i}. ID: {apt_id} - {url[:80]}...")
        if len(new_urls) > 10:
            print(f"   ... et {len(new_urls) - 10} autres")
    else:
        print("\n‚úÖ Aucune nouvelle URL (toutes √©taient d√©j√† dans l'historique)")
    
    print(f"\n‚úÖ TERMIN√â!")
    return all_urls

if __name__ == "__main__":
    main()

